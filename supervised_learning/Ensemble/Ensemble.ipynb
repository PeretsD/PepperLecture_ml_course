{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10556829",
   "metadata": {},
   "source": [
    "# Ансамблирование алгоритмов\n",
    "\n",
    "![Ансамбль](img/ansamble.jpg)\n",
    "\n",
    "**Ансамбль (Ensemble, Multiple Classifier System)** называется алгоритм, который состоит из нескольких алгоритмов машинного обучения, а процесс построения ансамбля называется ансамблированием (ensemble learning). \n",
    "\n",
    "Простейший пример ансамбля в регрессии – усреднение нескольких алгоритмов:\n",
    "\n",
    "$$ \\hat{y}(X) = \\frac{1}{n}\\{ model_{1}(X) + ... +  model_{n}(X) \\} $$\n",
    "\n",
    "Алгоритмы $ model_i(X) $ - базовые алгоритмы.\n",
    "\n",
    "$$ \\hat{y}(x) = mode(model_{1}(X), ... , model_{n}(X)) $$\n",
    "\n",
    "где  `mode` – мода.\n",
    "\n",
    "Обучаем несколько базовых моделей, а затем агрегируем их результаты по какому-либо правилу и выдаем окончательный результат.\n",
    "\n",
    "Зачем это нужно:\n",
    "\n",
    "* В совокупности получаем более сложную модель, чем каждая в отдельности\n",
    "* Избежание переобучения/недообучения\n",
    "* Возможность работы с признаками разной природы (использовать разные алгоритмы)\n",
    "\n",
    "Для простоты рассмотрим задачу бинарной классификации. Пусть всего $N$ базовых моделей и каждая предсказывает класс $c_{1}$ или $c_{2}$. Тогда агрегированный алгоритм может выдавать класс $c_{1}$ по следующим правилам:\n",
    "\n",
    "* AND-правило: если все базовые модели выдали $c_{1}$\n",
    "* OR-правило: если хотя бы одна базовая модель выдала $c_{1}$\n",
    "* $k-out-of-N$: если хотя бы $k$ базовых моделей из $N$ выдали $c_{1}$\n",
    "* majority vote: если большинство базовых моделей выдало $c_{1}$\n",
    "\n",
    "### Обобщение с весами\n",
    "Также, если используются правила $k-out-of-N$ или majority vote, можно каждой базовой модели присвоить вес, основываясь на качестве предсказания на валидационной выборке.\n",
    "\n",
    "### Предсказание класса по уровням ранжирования\n",
    "Пусть теперь рассматривается задачи многоклассовой классификации с $C$ классами. Пусть каждая $k$-ая базовая модель выдает некую отранжированную информацию о классе объекта:\n",
    "\n",
    "$c_{k_{1}} \\succeq c_{k_{2}} \\succeq \\dots \\succeq c_{k_{C}}$\n",
    "\n",
    "Это означает, что класс $c_{k_{1}}$ наиболее вероятен для рассматриваемого объекта, а класс {\\displaystyle c_{k_{C}}} --- наименее вероятен.\n",
    "\n",
    "Пусть $B_{k}(i)$ - сколько классов было отранжировано ниже $i$-го класса $k$-ой базовой моделью. Чем $B_{k}(i)$ выше, тем более вероятен $i$-ый класс. Поэтому, в качестве совокупного рейтинга построим следующую величину:\n",
    "\n",
    "$g_{i}(x)=\\sum \\limits_{k}B_{k}(i,x)$\n",
    "\n",
    "Тогда результирующее предсказание на объекте $x$:\n",
    "\n",
    "${\\hat {y}}(x)={\\underset {i\\in [1,\\dots ,C]}{\\operatorname {argmax} }}~~g_{i}(x)$\n",
    "\n",
    "### Предсказание класса по вероятностям\n",
    "Опять рассмотрим задачу многоклассовой классификации с $C$ классами. Пусть каждая $k$-ая базовая модель выдает вектор вероятностей из принадлежностей к каждому классу:\n",
    "\n",
    "$[p^{k}(c_{1}),p^{k}(c_{2}),\\dots ,p^{k}(c_{C})]$\n",
    "\n",
    "Тогда ${\\hat {y}}(x)=c_{i}$, где $i={\\underset {i\\in [1,\\dots ,C]}{\\operatorname {argmax} }}~~F(p^{1}(c_{i}),p^{2}(c_{i}),\\dots ,p^{N}(c_{i}))$\n",
    "\n",
    "$F$ - среднее арифметическое или медиана.\n",
    "\n",
    "\n",
    "### Номер класса\n",
    "Вектор вероятностей классов\n",
    "Любой изначальный или сгенерированный признак\n",
    "Бэггинг (Bagging)\n",
    "Генерируем $K$ выборок фиксированного размера $M$, выбирая с возвращением из $N$ имеющихся объектов. Доказывается, что каждый объект попадает в выборку с вероятностью $1-e^{-1}$, если $M=N$.\n",
    "\n",
    "Настраиваем $K$ базовых моделей на этих выборках и агрегируем результат.\n",
    "\n",
    "**Плюсы:**\n",
    "\n",
    "* Уменьшает переобучение, если базовые модели были переобучены (например, решающие деревья)\n",
    "\n",
    "**Минусы:**\n",
    "\n",
    "* Время обучения увеличивается в $K$ раз\n",
    "\n",
    "### Бутстреп\n",
    "\n",
    "Давайте начнем с определения `бутстрэпа`. Этот статистический метод заключается в генерации выборок размера $B$ (так называемых бутстрэп выборок) из исходного датасета размера N путем случайного выбора элементов с повторениями в каждом из наблюдений $B$.\n",
    "\n",
    "![bootstrap](img/bootstrap_1.png)\n",
    "\n",
    "При некоторых допущениях эти выборки имеют довольно хорошие статистические свойства: в первом приближении их можно рассматривать как взятые непосредственно из истинного базового (и часто неизвестного) распределения данных, так и независимо друг от друга. Таким образом, их можно рассматривать как репрезентативные и независимые выборки истинного распределения данных (почти идентичные выборки). Гипотеза, которая должна быть проверена, чтобы сделать это приближение действительным, имеет две стороны. Во-первых, размер N исходного датасета должен быть достаточно большим, чтобы охватить большую часть сложности базового распределения, чтобы выборка из датасета была хорошим приближением к выборке из реального распределения (репрезентативность). Во-вторых, размер датасета N должен быть достаточно большим по сравнению с размером бутстрэп выборок B, чтобы выборки не слишком сильно коррелировали (независимость). Обратите внимание, что в дальнейшем мы иногда будем ссылаться на эти свойства (репрезентативность и независимость) бутстрэп выборок: читатель всегда должен помнить, что это только приближение.\n",
    "\n",
    "Бутстрэп выборки часто используются, например, для оценки разброса или доверительных интервалов статистических оценок. По определению статистическая оценка является функцией некоторых наблюдений и, следовательно, случайной величины с разбросом, полученным из этих наблюдений. Чтобы оценить разброс такой оценки, нам нужно оценить его на нескольких независимых выборках, взятых из интересующего распределения. В большинстве случаев рассмотрение действительно независимых выборок потребовало бы слишком большого количества данных по сравнению с реально доступным количеством. Затем мы можем использовать бутстрэп, чтобы сгенерировать несколько бутстрэп выборок, которые можно рассматривать как «почти репрезентативные» и «почти независимые» (почти «независимые одинаково распределенные выборки»). Эти примеры бутстрэп выборок позволят нам аппроксимировать разброс оценки, оценивая его значение для каждой из них.\n",
    "\n",
    "### Бэггинг\n",
    "\n",
    "При обучении модели, независимо от того, имеем ли мы дело с проблемой `классификации` или `регрессии`, мы получаем функцию, которая принимает входные данные, возвращает выходные данные и определяется в отношении обучающего датасета. Из-за теоретического разброса обучающего датасета (мы напоминаем, что датасет является наблюдаемой выборкой, исходящей из истинно неизвестного базового распределения), подобранная модель также подвержена изменчивости: если бы наблюдался другой датасет, мы получили бы другую модель.\n",
    "\n",
    "Идея бэггинга в таком случае проста: мы хотим подобрать несколько независимых моделей и «усреднить» их прогнозы, чтобы получить модель с меньшим разбросом. Однако на практике мы не можем подобрать полностью независимые модели, потому что для этого потребуется слишком много данных. Таким образом, мы полагаемся на хорошие «приблизительные свойства» бутстрэп выборок (репрезентативность и независимость) для подбора моделей, которые практически независимы.\n",
    "\n",
    "![bagging](img/bagging.png)\n",
    "\n",
    "Сначала мы генерируем несколько бутстрэп выборок так, чтобы каждая новая бутстрэп выборка выполняла роль (почти) еще одного независимого датасета, взятого из истинного распределения. Затем мы можем обучить слабого ученика для каждой из этих выборок и, наконец, агрегировать их так, чтобы мы как бы «усреднили» их результаты и, таким образом, получили модель ансамбля с разбросом меньшим, чем ее отдельные компоненты. Грубо говоря, так как бутстрэп выборки являются примерно независимыми и одинаково распределенными, то же самое касается и обученных слабых учеников. Затем «усреднение» результатов слабых учеников не изменяет ожидаемый ответ, но уменьшает его разброс (так же, как усреднение независимых одинаково распределенных случайных величин сохраняет ожидаемое значение, но уменьшает разброс).\n",
    "\n",
    "Итак, предположим, что у нас есть $L$ бутстрап выборок (аппроксимации $L$ независимых датасетов) размера $B$. Это обозначается:\n",
    "\n",
    "$\\{z_{1}^{1}, z_{1}^{2}, ... , z_{1}^{B}\\}, \\{z_{2}^{1}, z_{2}^{2}, ... , z_{2}^{B}\\}, ..., \\{z_{L}^{1}, z_{L}^{2}, ... , z_{L}^{B}\\}$\n",
    "\n",
    "где $z_{B}^{L}$ - $B$-е наблюдение в $L$-й бутрстреп выборке\n",
    "\n",
    "Мы можем обучить $L$ почти независимых слабых учеников (по одному на каждый датасет):\n",
    "\n",
    "$\\omega_{1}(.), \\omega_{2}(.), ..., \\omega_{L}(.)$\n",
    "\n",
    "А затем объединим их некоторым процессом усреднения, чтобы получить модель ансамбля с меньшим разбросом. Например, мы можем определить нашу сильную модель так, чтобы\n",
    "\n",
    "$S_{L}(.)=\\frac{1}{L}\\sum_{i=1}^{L}\\omega_{i}(.)$ - простое среднее, для задачи регрессии\n",
    "\n",
    "$S_{L}(.)=arg max_{k}[card(l|\\omega_{l}(.)-k)]$ - голосование большинством, для задачи классификации\n",
    "\n",
    "\n",
    "Существует несколько возможных способов объединить несколько моделей, обученных параллельно. Для задачи регрессии выходные данные отдельных моделей могут быть буквально усреднены для получения выходных данных модели ансамбля. Для задачи `классификации` класс, предсказываемый каждой моделью, можно рассматривать как голос, а класс, который получает большинство голосов, является ответом модели ансамбля (это называется мажоритарным голосованием). Что касается задачи классификации, мы также можем рассмотреть вероятности каждого класса, предсказываемые всеми моделями, усреднить эти вероятности и сохранить класс с самой высокой средней вероятностью (это называется мягким голосованием). Средние значения или голоса могут быть простыми или взвешенными, если будут использоваться любые соответствующие им веса.\n",
    "\n",
    "Наконец, мы можем упомянуть, что одним из больших преимуществ `бэггинга` является его параллелизм. Поскольку различные модели обучаются независимо друг от друга, при необходимости могут использоваться методы интенсивного распараллеливания.\n",
    "\n",
    "### Бустинг\n",
    "\n",
    "Методы бустинга работают в том же духе, что и методы бэггинга: мы создаем семейство моделей, которые объединяются, чтобы получить сильного ученика, который лучше работает. Однако, в отличие от бэггинга, которое в основном направлено на уменьшение разброса, бустинг — это метод, который заключается в том, чтобы адаптировать последовательно нескольких слабых учеников адаптивным способом: каждая модель в последовательности подбирается, что придает большее значение объектам в датасете, которые плохо обрабатывались предыдущими моделями в последовательности. Интуитивно, каждая новая модель фокусирует свои усилия на самых сложных объектах выборки при обучении предыдущих моделей, чтобы мы получили в конце процесса сильного ученика с более низким смещением (даже если получится так, что бустинг будет при этом уменьшать разброс). Бустинг, как и бэггинг, может использоваться как для задач регрессии, так и для классификации.\n",
    "\n",
    "![boosting](img/boosting.png)\n",
    "\n",
    "Базовые модели, которые часто рассматриваются для бустинга — это модели с низким разбросом, но с высоким смещением. Например, если мы хотим использовать деревья решений в качестве наших базовых моделей, в основном мы будем выбирать неглубокие деревья решений с глубиной в несколько узлов. Другая важная причина, которая мотивирует использовать модели с низким разбросом, но с высоким смещением в качестве слабых учеников для бустинга, заключается в том, что эти модели, как правило, требуют меньших вычислительных затрат (несколько степеней свободы при подборе гиперпараметров). Действительно, поскольку вычисления для подгонки к различным моделям не могут выполняться параллельно (в отличие от бэггинга), это может стать слишком дорогостоящим для последовательного подбора нескольких сложных моделей.\n",
    "\n",
    "После того, как слабые ученики выбраны, нам все еще нужно определить, как они будут последовательно подгоняться (какую информацию из предыдущих моделей мы учитываем при подборе текущей модели?) И как они будут агрегироваться (как мы агрегируем текущую модель к предыдущим?). Мы обсудим эти вопросы в двух следующих подразделах, более подробно описывающих два важных алгоритма бустинга: adaboost (адаптивный бустинг) и градиентный бустинг.\n",
    "\n",
    "В двух словах, эти два мета-алгоритма отличаются тем, как они создают и объединяют слабых учеников в ходе последовательного процесса. Адаптивный бустинг обновляет веса, прикрепленные к каждому из объектов обучающего датасета, тогда как градиентный бустинг обновляет значения этих объектов. Эта разница исходит из того, что оба метода пытаются решить задачу оптимизации, заключающуюся в поиске наилучшей модели, которая может быть записана в виде взвешенной суммы слабых учащихся.\n",
    "\n",
    "#### Градиентный бустинг\n",
    "\n",
    "При градиентном бустинге модель ансамбля, которую мы пытаемся построить, представляет собой взвешенную сумму слабых учеников.\n",
    "\n",
    "$S_{L}(.)=\\sum_{i=1}^{L}c_{i}\\times \\omega_{i}(.)$, где $c_{i}$ - коэффициенты, $\\omega_{i}(.)$ - результаты слабых моделей.\n",
    "\n",
    "Найти оптимальную модель при этой форме записи модели ансамбля слишком сложно, и требуется итерационный подход. Градиентный бустинг сводит задачу к градиентному спуску: на каждой итерации мы подгоняем слабого ученика к антиградиенту текущей ошибки подбора по отношению к текущей модели ансамбля. Попробуем прояснить этот последний момент. Во-первых, теоретический процесс градиентного спуска по ансамблевой модели может быть записан как\n",
    "\n",
    "$s_{l}(.)=s_{l-1} - c_{l} \\times \\nabla_{s_{l-1}} E(s_{l-1})(.)$\n",
    "\n",
    "где $E(.)$ - ошибка подгонки данной модели, $c_{l}$ - коэффициент, соответствующий размеру шага, и\n",
    "\n",
    "$-\\nabla_{s_{l-1}}E(s_{l-1})(.)$\n",
    "\n",
    "является антиградиентом ошибки подгонки относительно модели ансамбля на шаге $l_{1}$. Этот (довольно абстрактный) антиградиент является функцией, которая на практике может оцениваться только для объектов в обучающей выборке (для которой мы знаем входные и выходные данные): эти оценки называются псевдо-остатками, прикрепленными к каждому объекту. Более того, даже если мы знаем для наблюдений значения этих псевдо-остатков, мы не хотим добавлять в нашу модель ансамбля какую-либо функцию: мы хотим добавить только новый экземпляр слабой модели. Таким образом, естественная вещь, которую нужно сделать, это научитьслабого ученика псевдо-остаткам для каждого наблюдения. Наконец, коэффициент $c_{l}$ вычисляется в соответствии с одномерным процессом оптимизации (линейный поиск для получения наилучшего размера шага $c_{l}$).\n",
    "\n",
    "Итак, предположим, что мы хотим использовать градиентный бустинг с семейством слабых моделей. В самом начале алгоритма (первая модель последовательности) псевдо-остатки устанавливаются равными значениям объектов. Затем мы повторяем $L$ раз (для $L$ моделей последовательности) следующие шаги:\n",
    "\n",
    "Обучить наилучшего возможного слабого ученика псевдо-остаткам (приблизить антиградиент по отношению к текущему сильному ученику)\n",
    "вычислить значение оптимального размера шага, который определяет, насколько мы обновляем модель ансамбля в направлении нового слабого ученика\n",
    "обновить модель ансамбля, добавив нового слабого ученика, умноженного на размер шага (сделать шаг градиентного спуска)\n",
    "вычислить новые псевдо-остатки, которые указывают для каждого наблюдения, в каком направлении мы хотели бы обновить следующие прогнозы модели ансамбля\n",
    "Повторяя эти шаги, мы последовательно строим наши $L$ моделей и агрегируем их в соответствии с подходом градиентного спуска. Обратите внимание на то, что, хотя адаптивный бустинг пытается решить на каждой итерации именно «локальную» задачу оптимизации (найти лучшего слабого ученика и его коэффициент, который нужно добавить к сильной модели), градиентный бустинг использует вместо этого подход с градиентным спуском и его легче адаптировать к большому количеству функций потерь.\n",
    "\n",
    "### Стэкинг моделей\n",
    "\n",
    "`Стекинг` имеет два основных отличия от бэггинга и бустинга. Во-первых, стекинг часто учитывает разнородных слабых учеников (комбинируются разные алгоритмы обучения), тогда как бэггинг и бустинг учитывают в основном однородных слабых учеников. Во-вторых, стекинг учит объединять базовые модели с использованием метамодели, тогда как бэггинг и бустинг объединяют слабых учеников с помощью детерминистическим алгоритмам.\n",
    "\n",
    "Как мы уже упоминали, идея стекинга состоит в том, чтобы выучить нескольких разных слабых учеников и объединить их, обучив метамодель для вывода предсказаний, основанных на множественных предсказаниях, возвращаемых этими слабыми моделями. Итак, нам нужно определить две вещи для построения нашей модели стека: $L$ учеников, которых мы хотим обучить, и метамодель, которая их объединяет.\n",
    "\n",
    "![stacking](img/stacking.png)\n",
    "\n",
    "Например, для задачи классификации мы можем в качестве слабого ученика выбрать классификатор KNN, логистическую регрессию и SVM и принять решение обучить нейронную сеть в качестве метамодели. Затем нейронная сеть примет в качестве входных данных результаты трех наших слабых учеников и научится давать окончательные прогнозы на их основе.\n",
    "\n",
    "Итак, предположим, что мы хотим обучить стековый ансамбль, составленный из $L$ слабых учеников. Затем мы должны выполнить следующие шаги:\n",
    "\n",
    "* разделить тренировочные данные на две части\n",
    "* выберите $L$ слабых учеников и обучите их на данных первого фолда (части)\n",
    "* для каждого из $L$ слабых учеников сделайте прогнозы для объектов из второго фолда\n",
    "* обучить метамодель во второй раз, используя в качестве входных данных прогнозы, сделанные слабыми учениками\n",
    "\n",
    "На предыдущих этапах мы разбили датасет на две части, потому что прогнозы данных, которые использовались для обучения слабых учеников, не имеют отношения к обучению метамодели. Таким образом, очевидным недостатком этого разделения нашего датасета на две части является то, что у нас есть только половина данных для обучения базовых моделей и половина данных для обучения метамодели. Чтобы преодолеть это ограничение, мы можем, однако, следовать некоторому подходу `k-fold кросс-обучение` (аналогичному тому, что делается в `k-fold кросс-валидации`), таким образом все объекты могут быть использованы для обучения мета-модели: для любого объекта предсказание слабых учеников делается на примерах этих слабых учеников, обученных на $k-1$ фолдах, которые не содержат рассматриваемого объекта. Другими словами, он обучается по $k-1$ фолдам, чтобы делать прогнозы для оставшегося фолда для объектов в любых фолдах. Таким образом, мы можем создать соответствующие прогнозы для каждого объекта нашего датасета, а затем обучить нашу метамодель всем этим прогнозам.\n",
    "\n",
    "Рассмотрим задачу регрессии. Пусть всего $K$ базовых моделей каждая модель - это $f_{k}(x)$ алгоритмов регрессии. Результирующую модель строим следующим образом:\n",
    "\n",
    "$f(x)=\\sum\\lim_{k=1}^{K}w_{k}f_{k}(x)$\n",
    "\n",
    "Можно находить веса следующим образом:\n",
    "\n",
    "${\\hat {w}}={\\underset {w}{\\operatorname {argmin} }}~\\sum \\limits _{i=1}^{N}{\\mathcal {L}}(y_{i},\\sum \\limits _{k=1}^{K}w_{k}f_{k}(x_{i}))$\n",
    "\n",
    "Но такой способ приведет к переобучению. Поэтому будем находить веса при помощи кросс-валидации, а именно: разобьем выборке на $M$ частей. Пусть $fold(i)$ - та часть, которая содержит $i$-ый объект, а $f_{k}^{-fold(i)}$ --- алгоритм, обученный на всех фолдах, кроме $fold(i)$. Тогда:\n",
    "\n",
    "${\\hat {w}}={\\underset {w}{\\operatorname {argmin} }}~\\sum \\limits _{i=1}^{N}{\\mathcal {L}}(y_{i},\\sum \\limits _{k=1}^{K}w_{k}f_{k}^{-fold(i)}(x_{i}))$\n",
    "\n",
    "Для уменьшения переобучения можно добавить условия на неотрицательность весов или добавить к функционалу регуляризатор $\\lambda \\sum \\limits _{k=1}^{K}(w_{k}-{\\dfrac {1}{K}})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d580d",
   "metadata": {},
   "source": [
    "## Бэггинг. Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffdc2f",
   "metadata": {},
   "source": [
    "`Случайный лес (Random forest)` — это композиция независимых деревьев решений. Ответы усредняются, если стоит задача регрессии и голосуется большинством, если задача классификации. \n",
    "Дерево принятия решений создается путем разделения данных на подмножества на используемые функции. \n",
    "\n",
    "Тренировка деревьев происходит независимо друг от друга (на разных подмножествах исходной выборки), что не просто решает проблему построения одинаковых деревьев на одном и том же наборе данных, но и делает этот алгоритм весьма удобным для применения в системах распределённых вычислений.\n",
    "\n",
    "Рассмотрим на примере задачи регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f0243",
   "metadata": {},
   "source": [
    "### Подключаем необходимые пакеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Деревья решений\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "# Случайный лес\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, confusion_matrix\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# библиотека для визуализации\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объем обучающей выборки = 300\n",
    "n_train = 500     \n",
    "# объём тестовой выборки = 1000\n",
    "n_test = 200\n",
    "# Влияние шума\n",
    "noise = 0.1\n",
    "\n",
    "# Аппроксимируемая функция\n",
    "def f(x):\n",
    "    return np.exp(-2*x ** 2) + 1.5 * np.exp(-(3*x - 5) ** 2) + 0.5 * np.sin(x)\n",
    "\n",
    "# Добавим к функции немного шума\n",
    "def f_noise(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-2*X ** 2) + 1.5 * np.exp(-(3*X - 5) ** 2) + 0.5 * np.sin(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "def plot_data(X, y):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(X,y)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model(X, y, title, labels):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for _ in range(y.shape[0]):\n",
    "        plt.scatter(X,y[_])\n",
    "        plt.title(title)\n",
    "        plt.legend(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc56f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация точек\n",
    "X_train, y_train = f_noise(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = f_noise(n_samples=n_test, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32210e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация данных\n",
    "plot_model(X_train, np.array([y_train]), 'train_data', 'f(x)')\n",
    "plot_model(X_test, np.array([y_test]), 'test_data', 'f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8308e1",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "Настроим модель `DecisionTreeRegressor` с разными параметрами `max_depth` для решения задачи регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tree_pred_list = []\n",
    "max_depth_list = [2,5,8,10]\n",
    "models_list = []\n",
    "\n",
    "for depth in max_depth_list:\n",
    "    # Создание объекта DecisionTreeRegressor\n",
    "    reg_tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    # Обучение модели\n",
    "    reg_tree.fit(X_train, y_train)\n",
    "    # Прогноз модели на X_test\n",
    "    reg_tree_pred_list.append(reg_tree.predict(X_test))\n",
    "    models_list.append(reg_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426c476",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Визуализация результатов\n",
    "plt.figure(figsize=(15, 12))\n",
    "# Тестовые точки\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "# Обучающая выборка\n",
    "plt.scatter(X_train, y_train, c=\"r\", s=5)\n",
    "\n",
    "for idx, reg_tree_pred in enumerate(reg_tree_pred_list):\n",
    "    plt.plot(X_test, reg_tree_pred, lw=1)\n",
    "\n",
    "plt.xlim([-5, 5])\n",
    "plt.legend(['test data','train data','depth=2','depth=5','depth=8','depth=10'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aefbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train = []\n",
    "metrics_test = []\n",
    "for depth in range(2,40):\n",
    "    # Создание объекта DecisionTreeRegressor\n",
    "    reg_tree = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    # Обучение модели\n",
    "    reg_tree.fit(X_train, y_train)\n",
    "    # Прогноз модели на X_test\n",
    "    metrics_train.append(reg_tree.score(X_train, y_train))\n",
    "    metrics_test.append(reg_tree.score(X_test, y_test))\n",
    "    \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(range(2,40),metrics_train, color='b',lw=2)\n",
    "plt.plot(range(2,40),metrics_test, color='r',lw=2)\n",
    "plt.legend(['train', 'test'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819aa75f",
   "metadata": {},
   "source": [
    "Как мы и изучали в теории, увеличение глубины дерева ведет к переобучению.\n",
    "\n",
    "> # Задание\n",
    ">\n",
    "> Какая глубина дерева наиболее оптимальна?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6abba",
   "metadata": {},
   "source": [
    "## RandomForestRegressor\n",
    "\n",
    "А теперь давайте посмотрим на объединение нескольких деревьев в случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest_pred_list = []\n",
    "max_depth_list = [3,7]\n",
    "n_est_list = [50,200]\n",
    "models_list = []\n",
    "\n",
    "for n_est in n_est_list:\n",
    "    for depth in max_depth_list:\n",
    "        # Создание объекта DecisionTreeRegressor\n",
    "        reg_forest = RandomForestRegressor(n_estimators=n_est,max_depth=depth, random_state=42)\n",
    "        # Обучение модели\n",
    "        reg_forest.fit(X_train, y_train)\n",
    "        # Прогноз модели на X_test\n",
    "        rand_forest_pred_list.append(reg_tree.predict(X_test))\n",
    "        models_list.append(reg_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация результатов\n",
    "plt.figure(figsize=(15, 12))\n",
    "# Тестовые точки\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "# Обучающая выборка\n",
    "plt.scatter(X_train, y_train, c=\"r\", s=5)\n",
    "\n",
    "for idx, rand_forest_pred in enumerate(rand_forest_pred_list):\n",
    "    plt.plot(X_test, rand_forest_pred, lw=1)\n",
    "\n",
    "plt.xlim([-5, 5])\n",
    "plt.legend(['test data','train data','d=3,est=50','d=3,est=200','d=7,est=50','d=7,est=200'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ef080",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_train = []\n",
    "metrics_test = []\n",
    "for n_est in [5,100,200]:\n",
    "    cur_metric_train = []\n",
    "    cur_metric_test = []\n",
    "    for depth in range(2,40):\n",
    "        # Создание объекта DecisionTreeRegressor\n",
    "        reg_rf = RandomForestRegressor(n_estimators=n_est,max_depth=depth, random_state=42)\n",
    "        # Обучение модели\n",
    "        reg_rf.fit(X_train, y_train)\n",
    "        # Прогноз модели на X_test\n",
    "        cur_metric_train.append(reg_rf.score(X_train, y_train))\n",
    "        cur_metric_test.append(reg_rf.score(X_test, y_test))\n",
    "        \n",
    "    metrics_train.append(cur_metric_train)\n",
    "    metrics_test.append(cur_metric_test)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(18,6))\n",
    "i=0\n",
    "for tr, te in zip(metrics_train, metrics_test):  \n",
    "    [ax1,ax2,ax3][i].plot(range(2,40),tr, color='b',lw=2)\n",
    "    [ax1,ax2,ax3][i].plot(range(2,40),te, color='r',lw=2)\n",
    "    [ax1,ax2,ax3][i].set_title(f'n_estimators={[5,100,200][i]}')\n",
    "    [ax1,ax2,ax3][i].legend(['train', 'test'])\n",
    "    [ax1,ax2,ax3][i].grid()\n",
    "    [ax1,ax2,ax3][i].set_ylim([0, 1])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9eda4",
   "metadata": {},
   "source": [
    "## Классификация\n",
    "\n",
    "Рассмотрим задачу классификации на привычном наборе данных про цветы Ириса\n",
    "\n",
    "### Датасет\n",
    "Рассматривать задачу будем на примере известного датасета **Цветки Ириса**\n",
    "\n",
    "Датасет [Цветки Ириса](https://archive.ics.uci.edu/ml/datasets/iris) содержит 150 записей, каждая из записей содержит 4 признака, т.е. $\\boldsymbol x \\in \\mathbb{R}^4$. \n",
    "\n",
    "Что за 4 признака?\n",
    "\n",
    "0. длина чашелистника, см\n",
    "1. ширина чашелистника, см\n",
    "2. длина лепестка, см \n",
    "3. ширина лепестка, см \n",
    "\n",
    "Метки классов\n",
    "\n",
    "0. Setosa\n",
    "1. Versicolour \n",
    "2. Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для отображения границ классов по модели\n",
    "def plot_contours(X_train, X_test, y_test, model, title):\n",
    "    h = .02  # шаг сетки\n",
    "    # Создадим сетку для отображения\n",
    "    x_min, x_max = X_test[:, 0].min()*0.9, X_test[:, 0].max()*1.1\n",
    "    y_min, y_max = X_test[:, 1].min()*0.9, X_test[:, 1].max()*1.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "    \n",
    "    y_pred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # \"Восстановим\" прогнозные точки на новой сетке\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.seismic, alpha=0.8)\n",
    "\n",
    "    # Добавим на график точки из датасета\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.seismic)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2] \n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим объект\n",
    "dtree_class = DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "\n",
    "# обучим модель\n",
    "dtree_class.fit(X_train, y_train)\n",
    "\n",
    "# визуализируем границы\n",
    "plot_contours(X_train, X_test, y_test, dtree_class, 'DecisionTree, max_depth=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим объект\n",
    "rf_class = RandomForestClassifier(n_estimators=2, criterion='entropy', max_depth=3)\n",
    "\n",
    "# обучим модель\n",
    "rf_class.fit(X_train, y_train)\n",
    "\n",
    "# визуализируем границы\n",
    "plot_contours(X_train, X_test, y_test, rf_class, 'Random Forest, n_est=10, max_depth=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Точность решающего дерева: {np.round(dtree_class.score(X_test, y_test),3)}')\n",
    "print(f'Точность случайного леса: {np.round(rf_class.score(X_test, y_test),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ff614",
   "metadata": {},
   "source": [
    "Добавив всего ещё одно дерево, разделяющая плоскость стала намного сложнее и метрика точности модели явно улучшилась."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc30609",
   "metadata": {},
   "source": [
    "### Теперь давайте посмотрим на основные параметры Случайного леса\n",
    "\n",
    "**Число деревьев — n_estimators**\n",
    "\n",
    "Увеличение количества деревьев влият на улучшение метрики, но в то же время время настройки и работы модели также  увеличиваются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4787e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_class_pred_list = []\n",
    "n_est_list = [2,4,6,10]\n",
    "models_list = []\n",
    "\n",
    "for n_est in n_est_list:\n",
    "    # Создание объекта RandomForest\n",
    "    rf_class = RandomForestRegressor(n_estimators=n_est,max_depth=depth, random_state=42)\n",
    "    # Обучение модели\n",
    "    rf_class.fit(X_train, y_train)\n",
    "    # Прогноз модели на X_test\n",
    "    rf_class_pred_list.append(rf_class.predict(X_test))\n",
    "    models_list.append(rf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # шаг сетки\n",
    "# Создадим сетку для отображения\n",
    "x_min, x_max = X_train[:, 0].min()*0.9, X_train[:, 0].max()*1.1\n",
    "y_min, y_max = X_train[:, 1].min()*0.9, X_train[:, 1].max()*1.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                 np.arange(y_min, y_max, h))\n",
    "\n",
    "f, axs = plt.subplots(2, 2, sharey=True, figsize=(12,11))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx, rf_class in enumerate(models_list):\n",
    "    y_pred = rf_class.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # \"Восстановим\" прогнозные точки на новой сетке\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    axs[idx].contourf(xx, yy, y_pred, cmap=plt.cm.seismic, alpha=0.8)\n",
    "    # Добавим на график точки из датасета\n",
    "    axs[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.seismic)\n",
    "    axs[idx].set_title(f'Точность test {np.round(rf_class.score(X_test, y_test),3)}, n_est = {n_est_list[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d3a68",
   "metadata": {},
   "source": [
    "`max_depth` - **максимальная глубина деревьев**\n",
    "\n",
    "При увеличении глубины возрастает качество на обучении и на тесте. Рекомендуется использовать максимальную глубину (кроме случаев, когда объектов слишком много и получаются очень глубокие деревья, построение которых занимает значительное время). При использовании неглубоких деревьев изменение параметров, связанных с ограничением числа объектов в листе и для деления, не приводит к значимому эффекту (листья и так получаются «большими»). Неглубокие деревья рекомендуют использовать в задачах с большим числом шумовых объектов (выбросов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873dbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = [2,3,4,5]\n",
    "models_list = []\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    # Создание объекта RandomForest\n",
    "    rf_class = RandomForestRegressor(n_estimators=5,max_depth=max_depth, random_state=42)\n",
    "    # Обучение модели\n",
    "    rf_class.fit(X_train, y_train)\n",
    "    models_list.append(rf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09109568",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = .02  # шаг сетки\n",
    "# Создадим сетку для отображения\n",
    "x_min, x_max = X_train[:, 0].min()*0.9, X_train[:, 0].max()*1.1\n",
    "y_min, y_max = X_train[:, 1].min()*0.9, X_train[:, 1].max()*1.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                 np.arange(y_min, y_max, h))\n",
    "\n",
    "f, axs = plt.subplots(2, 2, sharey=True, figsize=(12,11))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx, rf_class in enumerate(models_list):\n",
    "    y_pred = rf_class.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # \"Восстановим\" прогнозные точки на новой сетке\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    axs[idx].contourf(xx, yy, y_pred, cmap=plt.cm.seismic, alpha=0.8)\n",
    "    # Добавим на график точки из датасета\n",
    "    axs[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.seismic)\n",
    "    axs[idx].set_title(f'Точность test {np.round(rf_class.score(X_test, y_test),3)}, max_depth = {max_depth_list[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4adf21",
   "metadata": {},
   "source": [
    "**Число признаков для выбора расщепления — max_features**\n",
    "\n",
    "График качества на тесте от значения этого праметра унимодальный, на обучении он строго возрастает. При увеличении max_features увеличивается время построения леса, а деревья становятся «более однообразными». По умолчанию он равен sqrt(n) в задачах классификации и n/3 в задачах регрессии. Это самый важный параметр! Его настраивают в первую очередь (при достаточном числе деревьев в лесе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83923095",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_list = [1,2]\n",
    "models_list = []\n",
    "\n",
    "for max_features in max_features_list:\n",
    "    # Создание объекта RandomForest\n",
    "    rf_class = RandomForestRegressor(n_estimators=5,max_depth=3, max_features= max_features, random_state=42)\n",
    "    # Обучение модели\n",
    "    rf_class.fit(X_train, y_train)\n",
    "    models_list.append(rf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # шаг сетки\n",
    "# Создадим сетку для отображения\n",
    "x_min, x_max = X_train[:, 0].min()*0.9, X_train[:, 0].max()*1.1\n",
    "y_min, y_max = X_train[:, 1].min()*0.9, X_train[:, 1].max()*1.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                 np.arange(y_min, y_max, h))\n",
    "\n",
    "f, axs = plt.subplots(1, 2, sharey=True, figsize=(12,6))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx, rf_class in enumerate(models_list):\n",
    "    y_pred = rf_class.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # \"Восстановим\" прогнозные точки на новой сетке\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    axs[idx].contourf(xx, yy, y_pred, cmap=plt.cm.seismic, alpha=0.8)\n",
    "    # Добавим на график точки из датасета\n",
    "    axs[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.seismic)\n",
    "    axs[idx].set_title(f'Точность test {np.round(rf_class.score(X_test, y_test),3)}, max_features = {max_features_list[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc96300",
   "metadata": {},
   "source": [
    "**Минимальное число объектов, при котором выполняется расщепление — min_samples_split**\n",
    "\n",
    "Этот параметр, как правило, не очень важный и можно оставить значение по умолчанию (2). График качества на контроле может быть похожим на «расчёску» (нет явного оптимума). При увеличении параметра качество на обучении падает, а время построения RF сокращается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28392873",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split_list = [2,3,4,5]\n",
    "models_list = []\n",
    "\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    # Создание объекта RandomForest\n",
    "    rf_class = RandomForestRegressor(n_estimators=5,max_depth=3, min_samples_split=min_samples_split, random_state=42)\n",
    "    # Обучение модели\n",
    "    rf_class.fit(X_train, y_train)\n",
    "    models_list.append(rf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f290e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # шаг сетки\n",
    "# Создадим сетку для отображения\n",
    "x_min, x_max = X_train[:, 0].min()*0.9, X_train[:, 0].max()*1.1\n",
    "y_min, y_max = X_train[:, 1].min()*0.9, X_train[:, 1].max()*1.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                 np.arange(y_min, y_max, h))\n",
    "\n",
    "f, axs = plt.subplots(2, 2, sharey=True, figsize=(12,11))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx, rf_class in enumerate(models_list):\n",
    "    y_pred = rf_class.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # \"Восстановим\" прогнозные точки на новой сетке\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    axs[idx].contourf(xx, yy, y_pred, cmap=plt.cm.seismic, alpha=0.8)\n",
    "    # Добавим на график точки из датасета\n",
    "    axs[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.seismic)\n",
    "    axs[idx].set_title(f'Точность test {np.round(rf_class.score(X_test, y_test),3)}, min_samples_split = {min_samples_split_list[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda55bf",
   "metadata": {},
   "source": [
    "**Ограничение на число объектов в листьях — min_samples_leaf**\n",
    "\n",
    "Всё, что было описано про min_samples_split, годится и для описания этого параметра. Часто можно оставить значение по умолчанию (1). Кстати, по классике, в задачах регрессии рекомендуется использовать значение 5 (в библиотеке randomForest для R так и реализовано, в sklearn — 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723790ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf_list = [2,3,4,5]\n",
    "models_list = []\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_list:\n",
    "    # Создание объекта RandomForest\n",
    "    rf_class = RandomForestRegressor(n_estimators=5,max_depth=3, min_samples_leaf=min_samples_leaf, random_state=42)\n",
    "    # Обучение модели\n",
    "    rf_class.fit(X_train, y_train)\n",
    "    models_list.append(rf_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271bcafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # шаг сетки\n",
    "# Создадим сетку для отображения\n",
    "x_min, x_max = X_train[:, 0].min()*0.9, X_train[:, 0].max()*1.1\n",
    "y_min, y_max = X_train[:, 1].min()*0.9, X_train[:, 1].max()*1.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                 np.arange(y_min, y_max, h))\n",
    "\n",
    "f, axs = plt.subplots(2, 2, sharey=True, figsize=(12,11))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx, rf_class in enumerate(models_list):\n",
    "    y_pred = rf_class.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # \"Восстановим\" прогнозные точки на новой сетке\n",
    "    y_pred = y_pred.reshape(xx.shape)\n",
    "    axs[idx].contourf(xx, yy, y_pred, cmap=plt.cm.seismic, alpha=0.8)\n",
    "    # Добавим на график точки из датасета\n",
    "    axs[idx].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.seismic)\n",
    "    axs[idx].set_title(f'Точность test {np.round(rf_class.score(X_test, y_test),3)}, min_samples_leaf = {min_samples_leaf_list[idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7522f5e",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Вот и настало время посмотреть на самую распространенную в сфере ML/DS библиотеку для решения прикладных задач - [XGBoost](https://xgboost.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запустите код ниже, если библиотека XGboost не установлена\n",
    "#!pip install xgboost\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv as csv\n",
    "from xgboost import plot_importance, XGBRegressor, XGBRFClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "\n",
    "from scipy.stats import skew\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c99ee2",
   "metadata": {},
   "source": [
    "В основе `XGBoost` лежит алгоритм градиентного бустинга деревьев решений. Градиентный бустинг — это техника машинного обучения для задач классификации и регрессии, которая строит модель предсказания в форме ансамбля слабых предсказывающих моделей, обычно деревьев решений. Обучение ансамбля проводится последовательно в отличие, например от бэггинга. На каждой итерации вычисляются отклонения предсказаний уже обученного ансамбля на обучающей выборке. Следующая модель, которая будет добавлена в ансамбль будет предсказывать эти отклонения. Таким образом, добавив предсказания нового дерева к предсказаниям обученного ансамбля мы можем уменьшить среднее отклонение модели, которое является таргетом оптимизационной задачи. Новые деревья добавляются в ансамбль до тех пор, пока ошибка уменьшается, либо пока не выполняется одно из правил \"ранней остановки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aecd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объем обучающей выборки = 300\n",
    "n_train = 500     \n",
    "# объём тестовой выборки = 1000\n",
    "n_test = 200\n",
    "# Влияние шума\n",
    "noise = 0.1\n",
    "\n",
    "# Аппроксимируемая функция\n",
    "def f(x):\n",
    "    return np.exp(-2*x ** 2) + 1.5 * np.exp(-(3*x - 5) ** 2) + 0.5 * np.sin(x)\n",
    "\n",
    "# Добавим к функции немного шума\n",
    "def f_noise(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-2*X ** 2) + 1.5 * np.exp(-(3*X - 5) ** 2) + 0.5 * np.sin(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "    return X, y\n",
    "    \n",
    "def plot_model(X, y, title, labels):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for _ in range(y.shape[0]):\n",
    "        plt.scatter(X,y[_])\n",
    "        plt.title(title)\n",
    "        plt.legend(labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b90110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация точек\n",
    "X_train, y_train = f_noise(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = f_noise(n_samples=n_test, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ce03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация данных\n",
    "plot_model(X_train, np.array([y_train]), 'train_data', 'f(x)')\n",
    "plot_model(X_test, np.array([y_test]), 'test_data', 'f(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cc8cb",
   "metadata": {},
   "source": [
    "### XGBoostRegressor\n",
    "Настроим модель `XGBoostRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = XGBRegressor(max_depth=depth, random_state=42)\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a1245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg_pred_list = []\n",
    "max_depth_list = [2,3,4,6]\n",
    "models_list = []\n",
    "\n",
    "for depth in max_depth_list:\n",
    "    # Создание объекта DecisionTreeRegressor\n",
    "    xgb_reg = XGBRegressor(max_depth=depth, random_state=42)\n",
    "    # Обучение модели\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    # Прогноз модели на X_test\n",
    "    xgb_reg_pred_list.append(xgb_reg.predict(X_test))\n",
    "    models_list.append(xgb_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12d3ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Визуализация результатов\n",
    "plt.figure(figsize=(15, 12))\n",
    "# Тестовые точки\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "# Обучающая выборка\n",
    "plt.scatter(X_train, y_train, c=\"r\", s=5)\n",
    "\n",
    "for idx, xgb_reg_pred in enumerate(xgb_reg_pred_list):\n",
    "    plt.plot(X_test, xgb_reg_pred, lw=1)\n",
    "\n",
    "plt.xlim([-5, 5])\n",
    "plt.legend(['test data','train data','depth=2','depth=5','depth=8','depth=10'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9a350",
   "metadata": {},
   "source": [
    "> # Задание\n",
    ">\n",
    "> Проведите анализ по гиперпараметрам XGBoost для решения задачи регрессии по аналогии и исследованием RandomForest. Список гиперпараметров: `max_depth`, `n_estimators`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acba00",
   "metadata": {},
   "source": [
    "## XGBoost классификация\n",
    "\n",
    "Теперь применим `XGBoost` для задачи по цифрам `MNIST`. Предварительно снизим размерность точек до 2 и далее применим алгоритм `XGBoost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843667bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X_mnist = digits.data\n",
    "y_mnist = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим объект TSNE\n",
    "tSNE_mnist = TSNE(random_state=17)\n",
    "\n",
    "# настройка модели\n",
    "X_mnist_tsne = tSNE_mnist.fit_transform(X_mnist)\n",
    "\n",
    "# визуализация данных\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.scatter(X_mnist_tsne[:, 0], X_mnist_tsne[:, 1], c=y_mnist, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. t-SNE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRFClassifier(random_state=42, max_depth=2, n_estimators=10)\n",
    "\n",
    "X_2d_train, X_2d_test, y_train, y_test = train_test_split(X_mnist_tsne, y_mnist, test_size=.2, random_state=42)\n",
    "\n",
    "xgb.fit(X_2d_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contours(X_2d_train, X_2d_test, y_test, xgb, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5638941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем прогноз\n",
    "xgb.predict(X_2d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff925cbe",
   "metadata": {},
   "source": [
    "## Успешность алгоритма\n",
    "\n",
    "Посмотрим на матрицу неточностей. По оси Y - реальные метки, по X - результат модели. В идеале, все точки д.б. на главной диагонали, если какие-то значения вне главной диагонали, значит, классификатор не верно отнес цифру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(xgb.predict(X_2d_test),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6f6a7",
   "metadata": {},
   "source": [
    "> # Задание\n",
    ">\n",
    "> При помощи `GridSearchCV` определите оптимальный набор параметров для выделения цифр. Постройте графики точности от значения параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9902c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [2,5,7,10],\n",
    "    'n_estimators': [5,10,50,200], # add your parameters\n",
    "}\n",
    "\n",
    "xgb = XGBRFClassifier(random_state=42)\n",
    "\n",
    "# создаем объект GridSearchCV\n",
    "clf = GridSearchCV(xgb, param_grid=parameters, cv=3)\n",
    "\n",
    "# обучаем модель\n",
    "clf.fit(X_2d_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
