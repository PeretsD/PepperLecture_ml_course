{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание: <a id='chapter1'></a>\n",
    "1. [Основы теории вероятностей](#chapter2)  \n",
    "    1.1. [Случайные величины](#part2_1)  \n",
    "    1.2. [Свойства вероятности](#part2_2)  \n",
    "    1.3. [Условная вероятность](#part2_3)  \n",
    "    1.4. [Дискретные и непрерывные распределения](#part2_4)  \n",
    "2. [Примеры и задачи](#chapter3)\n",
    "3. [Основы статистики](#chapter4)  \n",
    "    3.1. [Генеральная совокупность и выборка](#part4_1)  \n",
    "    3.2. [Типы переменных.](#part4_2)  \n",
    "    3.3. [Выборочные оценки характеристик распределений](#part4_3)  \n",
    "    3.4. [Закон больших чисел и центральная предельная теорема](#part4_4)  \n",
    "    3.5. [Доверительные интервалы](#part4_5)  \n",
    "    3.6. [Идея статистического вывода, p-уровень значимости](#part4_6)  \n",
    "    3.7. [A/B тесты](#part4_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chapter2'></a>\n",
    "# 1. Основы теории вероятностей \n",
    "Предметом теории вероятностей является математический анализ случайных явлений — эмпирических феноменов, которые (при заданном «комплексе условий») могут быть охарактеризованы тем, что:\n",
    "- для них отсутствует детерминистическая регулярность (наблюдения над ними не всегда приводят к одним и тем же исходам)\n",
    "\n",
    "и в то же самое время:\n",
    "- они обладают некоторой статистической регулярностью (проявляющейся в статистической устойчивости частот).\n",
    "\n",
    "## 1.1. Случайные величины <a id='part2_1'></a>\n",
    "### Вероятностное пространство\n",
    "Вероятностное пространство — это тройка $(\\Omega,\\mathcal{F},P)$, где:\n",
    "\n",
    "$\\Omega=\\{\\omega_1, \\dots, \\omega_N\\}$ — это множество объектов $\\omega\\in\\Omega$, называемых элементарными исходами эксперимента. На это множество не накладывается никаких условий, оно может быть совершенно произвольным. При задании вероятностной модели для конкретного случайного эксперимента множество $\\Omega$ необходимо определять таким образом, чтобы в любой реализации опыта происходил один и только один элементарный исход. Элементарный исход содержит в себе всю возможную информацию о результате случайного опыта. С формальной математической точки зрения «произвести случайный опыт» означает в точности указать один элементарный исход $\\omega$, который произошел в данной реализации опыта.\n",
    "- *Пример.* При однократном подбрасывании монеты пространство исходов $\\Omega$ состоит из двух точек: $\\Omega = \\{Г, Р\\}$, где $Г$ — «герб», $Р$ — «решетка».\n",
    "\n",
    "$\\mathcal{F}$ — это некоторая зафиксированная система подмножеств $B\\subset\\Omega$, которые будут называться (случайными) событиями. Если элементарный исход, произошедший в результате реализации случайного опыта, входит в событие B, то говорят, что в данной реализации событие B произошло, иначе говорят, что событие не произошло. Совокупность событий $\\mathcal{F}$ должна быть сигма-алгеброй, то есть удовлетворять следующим свойствам:\n",
    "- Пустое множество $\\emptyset$ должно быть событием: $\\emptyset \\in \\mathcal{F}$. Это событие, которое существует в любом вероятностном пространстве, называется невозможным, поскольку оно никогда не происходит.\n",
    "- Все множество $\\Omega$ также должно быть событием: $\\Omega\\in\\mathcal{F}$. Это событие называется достоверным, так как происходит при любой реализации случайного опыта.\n",
    "- Совокупность событий $\\mathcal{F}$ должна образовывать алгебру, то есть быть замкнутой относительно основных теоретико-множественных операций, выполняемых над конечным числом событий. Если $A\\in\\mathcal{F}$ и $B\\in\\mathcal{F}$, тогда $A\\cup B\\in\\mathcal{F}, A\\cap B\\in\\mathcal{F}, A\\setminus B\\in\\mathcal{F}$. Операции над событиями имеют очевидный содержательный смысл.\n",
    "- В дополнение к указанным свойствам, система $\\mathcal{F}$ должна быть замкнута относительно операций над событиями, выполняемых в счетном числе (свойство сигма-алгебры). Если $\\{B_i\\}_{i=1}^\\infty\\subset\\mathcal{F}$, тогда $\\bigcup_{i=1}^\\infty B_i\\in\\mathcal{F}$ и $\\bigcap_{i=1}^\\infty B_i\\in\\mathcal{F}$.\n",
    "\n",
    "$P$ — это числовая функция, которая определена на $\\mathcal{F}$ и ставит в соответствие каждому событию $B\\in\\mathcal{F}$ число $P(B)$, которое называется вероятностью события B. Эта функция должна быть конечной сигма-аддитивной мерой, равной $1$ на всем пространстве, то есть удовлетворять условиям:\n",
    "- $0\\le P(\\omega_i)\\le 1, i=1,\\dots, N$\n",
    "- $P(\\omega_i)+\\dots+P(\\omega_N)=1$\n",
    "\n",
    "### Случайная величина\n",
    "Пусть задано вероятностное пространство $(\\Omega,\\mathcal{F},P)$. Случайной величиной, заданной на этом пространстве, называется числовая функция $\\xi:\\Omega\\to\\mathbb{R}$, которая ставит в соответствие каждому элементарному исходу $\\omega\\in\\Omega$ число $\\xi(\\omega)$ - значение случайной величины на этом исходе. Данная функция должна быть $\\mathcal{F}|\\mathcal{B}(\\mathbb{R})$-измеримой (где $\\mathcal{B}(\\mathbb{R}$) - борелевская сигма-алгебра на прямой), т.е. для любого борелевского множества $B\\in\\mathcal{B}(\\mathbb{R})$ его полный прообраз при отображении $\\xi$ должен быть событием: $\\xi^{-1}(B)\\in\\mathcal{F}$.\n",
    "\n",
    "#### Свойства\n",
    "\n",
    "Случайная величина может быть интерпретирована как некоторое измерение, в результате которого при каждой реализации случайного опыта мы получаем некоторое число.\n",
    "\n",
    "Случайная величина $\\xi$ индуцирует (порождает) новое вероятностное пространство $(\\mathbb{R},\\mathcal{B}(\\mathbb{R}),P_\\xi)$ с мерой $P_\\xi(B)=P(\\xi^{-1}(B))$, которая называется распределением вероятностей $\\xi$. Вероятность $P_\\xi(B)$ также обозначают $P(\\xi\\in B)$.\n",
    "\n",
    "Универсальный способ задания распределения случайной величины - через функцию распределения $F_\\xi(t)=P(\\xi<t)$\n",
    "\n",
    "Наиболее часто используемые типы случайных величин.\n",
    "\n",
    "В практических приложениях наиболее часто используются два типа случайных величин: дискретные и абсолютно непрерывные, хотя, разумеется, существуют случайные величины, не относящиеся ни к одному из этих классов.\n",
    "\n",
    "#### Дискретные случайные величины\n",
    "\n",
    "Дискретная случайная величина - это величина, принимающая конечное или счетное число значений. Такая величина задается набором этих значений $\\{\\xi_1,\\xi_2,\\ldots\\}$ и их вероятностей $\\{p_1,p_2,\\ldots\\}, p_i=P(\\xi=\\xi_i)$, которые должны быть неотрицательными и удовлетворять условию нормировки: $\\sum p_i=1$.\n",
    "\n",
    "При этом вероятностная мера на любом (борелевском) множестве прямой задается по формуле:\n",
    "\n",
    "$$P(\\xi\\in B)=\\sum_{i:\\xi_i\\in B}p_i, \\text{где} B\\in\\mathcal{B}(\\mathbb{R}).$$\n",
    "\n",
    "#### Абсолютно непрерывные случайные величины\n",
    "\n",
    "Если функция распределения случайной величины $\\xi$ имеет вид:\n",
    "\n",
    "$$F_\\xi(t)=P(\\xi<t)=\\int_{-\\infty}^t p(u)\\,du, $$\n",
    "где $p(u)$ - интегрируемая неотрицательная функция,\n",
    "тогда эта случайная величина называется абсолютно непрерывной. Функция $p(u)$ при этом называется плотностью распределения. Плотность распределения удовлетворяет свойствам:\n",
    "- $p(u)\\ge 0$\n",
    "- $\\int_{-\\infty}^\\infty p(u)\\,du=1.$\n",
    "\n",
    "И наоборот, любая интегрируемая функция $p(u)$, удовлетворяющая этим свойствам, может быть взята в качестве плотности распределения некоторой случайной величины. Поскольку функция распределения является функцией верхнего предела от плотности, то последняя восстанавливается по ней дифференцированием:\n",
    "$$p(t)=F'_\\xi(t).$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='part2_2'></a>\n",
    "## 1.2. Свойства вероятности\n",
    "1. Вероятность невозможного события (пустого множества $\\varnothing$) равна нулю:\n",
    "$$\\mathbf  {P}(\\varnothing )=0;$$\n",
    "Это следует из того, что каждое событие можно представить как сумму этого события и невозможного события, что в силу аддитивности и конечности вероятностной меры означает, что вероятность невозможного события должна быть равна нулю.\n",
    "\n",
    "2. $$\\mathbf  {P}(\\Omega)=0;$$\n",
    "3. Если событие A включается («входит») в событие B, то есть $A\\subset B$, то есть наступление события $A$ влечёт также наступление события $B$, то:\n",
    "$$\\mathbf  {P}(A)\\leqslant {\\mathbf  {P}}(B);$$\n",
    "Это следует из неотрицательности и аддитивности вероятностной меры, так как событие $B$, возможно, «содержит» кроме события $A$ ещё какие-то другие события, несовместные с $A$.\n",
    "\n",
    "4. Вероятность каждого события $A$ находится от $0$ до $1$, то есть удовлетворяет неравенствам:\n",
    "$$0\\leqslant {\\mathbf  {P}}(A)\\leqslant 1;$$\n",
    "Первая часть неравенства (неотрицательность) утверждается аксиоматически, а вторая следует из предыдущего свойства с учётом того, что любое событие «входит» в $X$, а для $X$ аксиоматически предполагается $\\mathbf  {P}(X)=1$.\n",
    "\n",
    "5. Вероятность наступления события $B\\setminus A$, где $A\\subset B$, заключающегося в наступлении события $B$ при одновременном ненаступлении события $A$, равна:\n",
    "$$\\mathbf  {P}(B\\setminus A)={\\mathbf  {P}}(B)-{\\mathbf  {P}}(A);$$\n",
    "Это следует из аддитивности вероятности для несовместных событий и из того, что события $A$ и $B\\setminus A$ являются несовместными по условию, а их сумма равна событию $B$.\n",
    "\n",
    "6. Вероятность события $\\bar  {A}$, противоположного событию $A$, равна:\n",
    "$$\\mathbf  {P}({\\bar  {A}})=1-{\\mathbf  {P}}(A);$$\n",
    "Это следует из предыдущего свойства, если в качестве множества $B$ использовать всё пространство $X$ и учесть, что $\\mathbf  {P}(X)=1$.\n",
    "\n",
    "7. *Теорема сложения вероятностей.* Вероятность наступления хотя бы одного из (то есть суммы) произвольных (не обязательно несовместных) двух событий $A$ и $B$ равна:\n",
    "$$\\mathbf  {P}(A+B)={\\mathbf  {P}}(A)+{\\mathbf  {P}}(B)-{\\mathbf  {P}}(AB).$$\n",
    "Это свойство можно получить, если представить объединение двух произвольных множеств как объединение двух непересекающихся — первого и разности между вторым и пересечением исходных множеств: $A+B=A+(B\\setminus (AB))$. Отсюда учитывая аддитивность вероятности для непересекающихся множеств и формулу для вероятности разности (см. свойство 4) множеств, получаем требуемое свойство.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2_3'></a>\n",
    "[наверх](#chapter1)\n",
    "## 1.3. Условная вероятность\n",
    "### Формула Байеса\n",
    "Вероятность наступления события $A$, при условии наступления события $B$, называется условной вероятностью $A$ (при данном условии) и обозначается $P(A\\mid B)$. \n",
    "\n",
    "Наиболее просто вывести формулу определения условной вероятности исходя из классического определения вероятности. Для данных двух событий $A$ и $B$ рассмотрим следующий набор несовместных событий: $A\\overline {B},AB,\\overline {A}B,\\overline {A}\\cdot \\overline {B}$, которые исчерпывают все возможные варианты исходов (такой набор событий называют полным). \n",
    "\n",
    "Общее количество равновозможных исходов равно $n$. Если событие $B$ уже наступило, то равновозможные исходы ограничивается лишь двумя событиями $AB,\\overline {A}B$, что эквивалентно событию $B$. Пусть количество этих исходов равно $n_{B}$. Из этих исходов событию $A$ благоприятствуют лишь те, что связаны с событием $AB$. Количество соответствующих исходов обозначим $n_{{AB}}$. Тогда согласно классическому определению вероятности вероятность события $A$ при условии наступления события $B$ будет равна $P(A\\mid B)=n_{AB}/n_{B}$, разделив числитель и знаменатель на общее количество равновозможных исходов $n$ и повторно учитывая классическое определение, окончательно получим **формулу условной вероятности**:\n",
    "\n",
    "$$P(A\\mid B)={\\frac {P(AB)}{P(B)}}.$$\n",
    "Отсюда следует так называемая теорема умножения вероятностей:\n",
    "\n",
    "$$P(AB)=P(B)\\cdot P(A\\mid B).$$\n",
    "В силу симметрии, аналогично можно показать, что также $P(AB)=P(A)\\cdot P(B\\mid A)$, отсюда следует **формула Байеса**:\n",
    "$$P(A\\mid B)={\\frac {P(A)\\cdot P(B\\mid A)}{P(B)}}$$\n",
    "\n",
    "### Независимость событий\n",
    "События A и B называются независимыми, если вероятность наступления одного из них не зависит от того, наступило ли другое событие. С учётом понятия условной вероятности это означает, что $\\displaystyle P(A\\mid B)=P(A)$, откуда следует, что для независимых событий выполняется равенство\n",
    "\n",
    "$$\\displaystyle P(AB)=P(A)\\cdot P(B).$$\n",
    "В рамках аксиоматического подхода данная формула принимается как определение понятия независимости двух событий. Для произвольной (конечной) совокупности событий $A_{i}$ их независимость в совокупности означает, что вероятность их совместного наступления равна произведению их вероятностей:\n",
    "\n",
    "$$P(A_{1}A_{2}\\dotsb A_{n})=P(A_{1})P(A_{2})\\dotsb P(A_{n}).$$\n",
    "Выведенная выше формула условной вероятности при аксиоматическом определении вероятности является определением условной вероятности. Соответственно, как следствие определений независимых событий и условной вероятности, получается равенство условной и безусловной вероятностей события.\n",
    "\n",
    "### Полная вероятность и формула Байеса\n",
    "Набор событий $A_{i}$, хотя бы одно из которых обязательно (с единичной вероятностью) наступит в результате испытания, называется полным. Это означает, что набор таких событий исчерпывает все возможные варианты исходов. Формально в рамках аксиоматического подхода это означает, что $\\sum _{i}A_{i}=X$. Если эти события несовместны, то в рамках классического определения это означает, что сумма количеств элементарных событий, благоприятствующих тому или иному событию, равно общему количеству равновозможных исходов.\n",
    "\n",
    "Пусть имеется полный набор попарно несовместных событий $A_{i}$. Тогда для любого события $B$ верна следующая формула расчёта его вероятности (**формула полной вероятности**):\n",
    "\n",
    "$$P(B)=\\sum _{i=1}^{n}P(B\\mid A_{i})P(A_{i})$$\n",
    "Тогда вышеописанную **формулу Байеса** с учётом полной вероятности можно записать в следующем виде:\n",
    "\n",
    "$${\\displaystyle P(A_{j}\\mid B)={\\frac {P(A_{j})\\cdot P(B\\mid A_{j})}{\\sum _{i=1}^{n}P(A_{i})\\cdot P(B\\mid A_{i})}}}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='part2_4'></a>\n",
    "## 1.4. Дискретные и непрерывные распределения\n",
    "\n",
    "### Дискретные случайные величины\n",
    "#### Дискретное равномерное распределение $U(a,b)$\n",
    "Случайная величина $\\xi$ имеет дискретное равномерное распределение, если она принимает конечное число $n$ значений с равными вероятностями, соответственно, вероятность каждого значения равна $\\frac{1}{n}$.\n",
    "\n",
    "<img src=\"img/DUniform_distribution.svg\" width=\"250\">\n",
    "<center>Функция вероятности ($n=5$)</center>\n",
    "\n",
    "#### Распределение Бернулли $Ber(p)$\n",
    "Распределение Бернулли - дискретное распределение вероятностей, моделирующее случайный эксперимент произвольной природы, при заранее известной вероятности успеха или неудачи.\n",
    "\n",
    "Случайная величина $\\xi$ имеет распределение Бернулли с параметром $p$, если $\\xi$ принимает значения $1$ и $0$ с вероятностями $p$ и $1-p$ соответственно. Случайная величина $\\xi$ с таким распределением равна числу успехов в одном испытании схемы Бернулли с вероятностью успеха $\\xi$: ни одного успеха или один успех.\n",
    "$$P(\\xi=1)=p,$$\n",
    "$$P(\\xi=0)=1-p = q.$$\n",
    "\n",
    "<img src=\"img/Bernuilli_distribution_PDF.svg\" width=\"250\">\n",
    "<center>Функция вероятности</center>\n",
    "\n",
    "#### Биномиальное распределение $Bin(n, p)$\n",
    "\n",
    "**В чем постановка задачи?**\n",
    "\n",
    "Представим себе испытание с двумя возможными исходами: $А$ и $\\overline A$, где, скажем, $А$ условно означает «успех», дополнительное событие $\\overline А$ – «неудачу».\n",
    "\n",
    "Серию независимых испытаний такого рода с одной и той же вероятностью успеха $р=Р(А)$ называют **испытаниями Бернулли**.\n",
    "\n",
    "Примером может служить последовательное бросание монеты, в котором условно выпадение герба есть успех, а выпадение решетки – неудача.\n",
    "\n",
    "Биномиальное распределение – закон распределения случайной величины равной числу успехов в испытаниях Бернулли. Эта случайная величина $\\xi$ может принимать любое из значений $0, 1, \\dots, n$, а их вероятность определяется формулой Бернулли: если $p$ – вероятность успеха, $q$ – вероятность неудачи, $q=1-p$, то\n",
    "$$P(\\xi = k)=P_n(k)=C_n^k p^k q^{n-k},$$\n",
    "где:\n",
    "- n = общее количество проведённых испытаний\n",
    "- k = количество успехов\n",
    "- $P_n(k)$ = вероятность того, что из $n$ испытаний $k$ будет «успешными»\n",
    "- $C_n^k= \\frac{n!}{k!(n−k)!}$ – биномиальный коэффициент\n",
    "\n",
    "<img src=\"img/Binomial_distribution.svg\" width=\"400\">\n",
    "<center>Функция вероятности</center>\n",
    "\n",
    "#### Распределение Пуассона $P(\\lambda)$\n",
    "\n",
    "**В чем постановка задачи?**\n",
    "\n",
    "Проводится $n$ независимых испытаний, в каждом из которых случайное событие $A$ может появиться с вероятностью $p$. Требуется найти вероятность того, что в данной серии испытаний событие $A$ появится ровно $m$ раз.\n",
    "\n",
    "Распределение Пуассона — распределение дискретного типа случайной величины, представляющей собой число событий, произошедших за фиксированное время, при условии, что данные события происходят с некоторой фиксированной средней интенсивностью $\\lambda$ и независимо друг от друга.\n",
    "$$P(\\xi = k)=\\frac{\\lambda ^k}{k!}e^{-\\lambda}, k=0,1,2,\\dots, \\lambda>0,$$\n",
    "где\n",
    "- $\\lambda$ – математическое ожидание случайной величины (среднее количество событий за фиксированный промежуток времени).\n",
    "- $\\lambda = n*p$, где:\n",
    "    - $n$ – количество независимых испытаний\n",
    "    - $p$ – вероятность появления события Х\n",
    "    - $k$ – количество успешных исходов события в конкретной серии испытаний\n",
    "<img src=\"img/Poisson_distribution_PMF.png\" width=\"350\">\n",
    "<center>Функция вероятности</center>\n",
    "\n",
    "\n",
    "### Непрерывные случайные величины\n",
    "#### Непрерывное равномерное распределение $U(a,b)$\n",
    "Равномерное распределение - распределение случайной вещественной величины, принимающей значения, принадлежащие некоторому промежутку конечной длины, характеризующееся тем, что плотность вероятности на этом промежутке почти всюду постоянна.\n",
    "\n",
    "Непрерываная случайная величина $\\xi$ имеет равномерное распределение на отрезке $[a, b]$, если плотность распределения $p_\\xi (x)$ сохраняет постоянное значение на этом промежутке:\n",
    "    $$p=\n",
    "    \\begin{equation}\n",
    "    \\begin{cases}\n",
    "      \\frac{1}{b-a}, &x \\in [a,b] \\\\\n",
    "      0, &x \\notin [a,b]\n",
    "    \\end{cases}\\,.\n",
    "\\end{equation}$$\n",
    "<img src=\"img/Uniform_distribution.svg\" width=\"250\">\n",
    "<center>Плотность вероятности</center>\n",
    "\n",
    "#### Нормальное распределение (распределение Гауса) $N(\\mu, \\sigma ^2)$\n",
    "Плотность распределения:\n",
    "$$p_\\xi (x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "$N(0, 1)$ - стандартное нормальное распределение. Стандартным нормальным распределением называется нормальное распределение с математическим ожиданием $\\mu =0$ и стандартным отклонением $\\sigma =1$.\n",
    "<img src=\"img/Normal_distribution_pdf.png\" width=\"400\">\n",
    "<center>Плотность вероятности</center>\n",
    "\n",
    "Если величина является суммой многих случайных слабо взаимозависимых величин, каждая из которых вносит малый вклад относительно общей суммы, то центрированное и нормированное распределение такой величины при достаточно большом числе слагаемых стремится к нормальному распределению.\n",
    "\n",
    "Это следует из центральной предельной теоремы теории вероятностей. В окружающем нас мире часто встречаются величины, значение которых определяется совокупностью многих независимых факторов. Этот факт, а также то, что распределение считалось типичным, обычным, привели к тому, что в конце XIX века стал использоваться термин «нормальное распределение». Нормальное распределение играет заметную роль во многих областях науки, например в математической статистике и статистической физике.\n",
    "\n",
    "Случайная величина, имеющая нормальное распределение, называется нормальной, или гауссовской, случайной величиной.\n",
    "\n",
    "**Стандартное нормальное распределение**\n",
    "\n",
    "\n",
    "Наиболее простой случай нормального распределения — стандартное нормальное распределение — частный случай.\n",
    "\n",
    "$$\\varphi (x)={\\frac {1}{\\sqrt {2\\pi }}}e^{-{\\frac {1}{2}}x^{2}}$$\n",
    "Множитель ${\\frac {1}{\\sqrt {2\\pi }}}$ в выражении обеспечивает условие нормировки интеграла $\\int \\limits _{-\\infty }^{+\\infty }\\varphi (x)\\,dx=1$. Поскольку множитель ${\\frac {1}{2}}$ в экспоненте обеспечивает дисперсию равную единице, то и стандартное отклонение равно 1. Функция симметрична в точке $x=0$, её значение в ней максимально и равно ${\\frac {1}{\\sqrt {2\\pi }}}$ Точки перегиба функции: $x=+1$ и $x=-1$\n",
    "\n",
    "Гаусс называл стандартным нормальным распределение с $\\sigma ^{2}=1/2$, то есть:\n",
    "\n",
    "$$\\varphi (x)={\\frac {e^{-x^{2}}}{\\sqrt {\\pi }}}$$\n",
    "\n",
    "#### Логнормальное распределение\n",
    "Двухпараметрическое семейство абсолютно непрерывных распределений. Если случайная величина имеет логнормальное распределение, то её логарифм имеет нормальное распределение.\n",
    "\n",
    "<img src=\"img/Lognormal_distribution_PDF.png\" width=\"400\">\n",
    "<center>Плотность вероятности</center>\n",
    "\n",
    "Пусть распределение случайной величины $X$ задаётся плотностью вероятности, имеющей вид:\n",
    "\n",
    "$$f_{X}(x)={\\frac {1}{x\\sigma {\\sqrt {2\\pi }}}}e^{-(\\ln x-\\mu )^{2}/2\\sigma ^{2}}$$\n",
    "\n",
    "где $x>0,\\;\\sigma >0,\\;\\mu \\in \\mathbb {R}$. Тогда говорят, что $X$ имеет логнормальное распределение с параметрами $\\mu$ и $\\sigma$. Это распределение записывают как: $X\\sim \\mathrm {LogN} (\\mu ,\\sigma ^{2})$.\n",
    "\n",
    "**Моменты**\n",
    "\n",
    "Формула для $k$-го момента логнормальной случайной величины $X$ имеет вид:\n",
    "\n",
    "$$\\mathbb {E} \\left[X^{k}\\right]=e^{k\\mu +{\\frac {k^{2}\\sigma ^{2}}{2}}},\\;k\\in \\mathbb {N}$$\n",
    "\n",
    "откуда в частности:\n",
    "\n",
    "$$\\mathbb {E} [X]=e^{\\mu +{\\sigma ^{2} \\over 2}}$$,\n",
    "$$\\mathrm {D} [X]=\\left(e^{\\sigma ^{2}}-1\\right)e^{2\\mu +\\sigma ^{2}}$$.\n",
    "\n",
    "Любые нецентральные моменты n-мерного совместного логнормального распределения могут быть вычислены по простой формуле:\n",
    "\n",
    "$$\\alpha _{n}=e^{(\\mu ,n)+{\\frac {1}{2}}(n,\\Sigma n)}$$, \n",
    "\n",
    "где $\\mu$  и $\\Sigma$  — параметры многомерного совместного распределения. $n$ — вектор, компоненты которого задают порядок момента. (Например, в двухмерном случае, $n=(2,0)$ — второй нецентральный момент первой компоненты, $n=(1,1)$ — смешанный второй момент). Круглые скобки обозначают скалярное произведение.\n",
    "\n",
    "#### Распределение $\\chi ^{2}$\n",
    "Распределение $\\chi ^{2}$ с $k$ степенями свободы — это распределение суммы квадратов $k$ независимых стандартных нормальных случайных величин.\n",
    "\n",
    "Пусть $\\xi_1, \\dots, \\xi_k$ — совместно независимые стандартные нормальные случайные величины, то есть: $\\xi_i \\sim N(0,1)$. Тогда случайная величина $\\eta = \\xi_1^2+ \\dots+ \\xi_k^2$ имеет распределение хи-квадрат с $k$ степенями свободы, то есть \n",
    "$$\\eta=\\sum_{i=1}^{k}\\xi_{i}^{2}\\sim \\chi ^{2}(k)$$.\n",
    "Плотность распределения хи-квадрат имеет вид:\n",
    "$$f_{\\chi ^{2}(k)}(x)={\\frac {(1/2)^{k \\over 2}}{\\Gamma \\!\\left({k \\over 2}\\right)}}\\,x^{{k \\over 2}-1}\\,e^{-{\\frac {x}{2}}},$$\n",
    "где гамма-функция $\\Gamma (z)=\\int \\limits _{0}^{+\\infty }t^{z-1}e^{-t}\\,dt,\\quad z\\in \\mathbb {C} ,\\quad \\mathrm {Re} (z)>0$.\n",
    "\n",
    "<img src=\"img/Chi-square_distributionPDF.png\" width=\"400\">\n",
    "<center>Плотность вероятности</center>\n",
    "\n",
    "#### Распределение Стьюдента\n",
    "Распределение Стьюдента играет важную роль в статистическом анализе и используется, например, в t-критерии Стьюдента для оценки статистической значимости разности двух выборочных средних, при построении доверительного интервала для математического ожидания нормальной совокупности при неизвестной дисперсии, а также в линейном регрессионном анализе.\n",
    "\n",
    "График плотности распределения Стьюдента, как и нормального распределения, является симметричным и имеет вид колокола, но с более «тяжёлыми» хвостами, то есть реализациям случайной величины, имеющей распределение Стьюдента, более свойственно сильно отличаться от математического ожидания.\n",
    "\n",
    "Пусть $\\xi_0, \\dots, \\xi_n$ — независимые стандартные нормальные случайные величины, такие что $\\xi_{i}\\sim {\\mathcal {N}}(0,1),\\;i=0,\\ldots ,n$. Тогда распределение случайной величины $\\eta$, где\n",
    "$$\\eta=\\frac{\\xi_0}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n\\xi_i^2}},$$\n",
    "называется рапределением Стьюдента с $n$ степенями свободы $\\eta \\sim {\\mathrm  {t}}(n).$\n",
    "\n",
    "Это распределение абсолютно непрерывно с плотностью:\n",
    "$$f_{t}(y)={\\frac {\\Gamma \\left({\\frac {n+1}{2}}\\right)}{{\\sqrt {n\\pi }}\\,\\Gamma \\left({\\frac {n}{2}}\\right)}}\\,\\left(1+{\\frac {y^{2}}{n}}\\right)^{-{\\frac {n+1}{2}}},$$\n",
    "где $\\Gamma$  — гамма-функция Эйлера.\n",
    "\n",
    "<img src=\"img/Student_densite_best.jpg\" width=\"400\">\n",
    "<center>Плотность вероятности</center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='chapter3'></a>\n",
    "# 2. Примеры и задачи\n",
    "## Примеры\n",
    "### Дискретное равномерное распределение\n",
    "1. При подбрасывании монеты случайная величина принимает значение $1$, если выпал «орёл», или $0$, если выпала «решка». Вероятность выпадения одного из двух значений равна $\\frac{1}{2}$, одинакова для обоих значений, поэтому случайная величина имеет дискретное равномерное распределение.\n",
    "2. При бросании игральной кости случайная величина — число точек на грани принимает одно из 6-и возможных значений:$\\{1,2,3,4,5,6\\}$. Вероятность выпадения одной точки из шести равна $\\frac{1}{6}$, одинакова для каждой точки, поэтому случайная величина имеет дискретное равномерное распределение.\n",
    "***\n",
    "### Биномиальное распределение\n",
    "1. Вероятность выпадения 2-х решек при 10 подбрасываниях монеты.\n",
    "$$P_{10}(2)=C_{10}^2 \\left(\\frac{1}{2}\\right)^2 \\left(\\frac{1}{2}\\right)^{10-2} = \\frac{10!}{2! (10-2)!} \\cdot \\frac{1}{4} \\cdot \\frac{1}{256} = \\frac{9 \\cdot 10}{2 \\cdot 4 \\cdot 256} \\approx 0.0439$$\n",
    "2. Вероятность выпадения 3-x очков 4 раза при 7 подбрасываниях кубика.\n",
    "    - $n=7$ к-во подбрасываний\n",
    "    - $p=\\frac{1}{6}$ вероятность выпадения 3 очков\n",
    "    - $q=1-\\frac{1}{6} = \\frac{5}{6}$\n",
    "    - $k=4$\n",
    "$$P_7(4)=C_{7}^4 \\left(\\frac{1}{6}\\right)^4 \\left(\\frac{5}{6}\\right)^{7-4} = \\frac{7!}{4! (7-4)!} \\left(\\frac{1}{6}\\right)^4 \\left(\\frac{5}{6}\\right)^{3} = \\frac{5 \\cdot 6 \\cdot 7 \\cdot 5^3}{6 \\cdot 6^4 \\cdot 6^3} = \\frac{5^4 \\cdot 7}{6^{7}} \\approx 0.0156$$\n",
    "***\n",
    "### Формула полной вероятности\n",
    "1. В двух урнах находится соответственно 4 и 5 белых и 6 и 3 чёрных шаров. Из каждой урны наудачу извлекается один шар, а затем из этих двух наудачу берется один. Какова вероятность, что это будет белый шар?\n",
    "\n",
    "**Решение**\n",
    "\n",
    "$$P(B)=\\sum _{i=1}^{n}P(B\\mid A_{i})P(A_{i})$$\n",
    "\n",
    "Пусть:\n",
    "- $A_1 =$ из первой урны вытащили белый шар, из второй вытащили черный шар,\n",
    "- $A_2 =$ из первой урны вытащили белый шар, из второй вытащили белый шар,\n",
    "- $A_3 =$ из первой урны вытащили черный шар, из второй вытащили черный шар,\n",
    "- $A_4 =$ из первой урны вытащили черный шар, из второй вытащили белый шар.\n",
    "- $B =$ из этих двух вынули наудачу белый шар. \n",
    "\n",
    "Найдем вероятности гипотез по классическому определению вероятности.\n",
    "\n",
    "$$P(A_1)=\\frac{4}{4+6} \\cdot \\frac{3}{5+3} = \\frac{12}{80}$$\n",
    "\n",
    "$$P(A_2)=\\frac{4}{4+6} \\cdot \\frac{5}{5+3} = \\frac{20}{80}$$\n",
    "\n",
    "$$P(A_3)=\\frac{6}{4+6} \\cdot \\frac{3}{5+3} = \\frac{18}{80}$$\n",
    "\n",
    "$$P(A_4)=\\frac{6}{4+6} \\cdot \\frac{5}{5+3} = \\frac{30}{80}$$\n",
    "\n",
    "Подсчитаем априорные условные вероятности:\n",
    "\n",
    "$$P(B \\mid A_1) = P(B \\mid A_4) = \\frac{1}{2}, P(B \\mid A_2)=1, P(B \\mid A_3)=0$$\n",
    "\n",
    "Вероятность события $B$ найдем по формуле полной вероятности:\n",
    "$P(B)=P(B\\mid A_1)P(A_1) + P(B\\mid A_2)P(A_2) + P(B\\mid A_3)P(A_3) + P(B\\mid A_4)P(A_4) = \\frac{12}{80} \\cdot \\frac{1}{2} + \\frac{20}{80} \\cdot 1 + \\frac{18}{80} \\cdot 0 + \\frac{30}{80} \\cdot \\frac{1}{2} \\approx 0.51$\n",
    "\n",
    "***\n",
    "2. Из 1000 ламп 380 принадлежат к 1 партии, 270 – ко второй партии, остальные к третьей. В первой партии 4% брака, во второй - 3%, в третьей – 6%. Наудачу выбирается одна лампа. Определить вероятность того, что выбранная лампа – бракованная.\n",
    "\n",
    "**Решение**\n",
    "\n",
    "$A_i =$ Лампа принадлежат $i$-ой партии, $i=1,2,3$.\n",
    "\n",
    "Всего ламп 1000, из них 1-ой партии принадлежат 380 $\\Rightarrow P(A_1) = \\frac{380}{1000}, P(A_2) = \\frac{270}{1000}$. \n",
    "\n",
    "$1000-380-270=350$ ламп принадлежат 3-ей партии $\\Rightarrow P(A_3) = \\frac{350}{1000}$\n",
    "\n",
    "Введем событие $B =$ лампа бракованная. По условию даны априорные вероятности:\n",
    "$P(B \\mid A_1) = 0.04 , P(B \\mid A_2) = 0.03, P(B \\mid A_3) = 0.06$.\n",
    "\n",
    "Вероятность события $B$ найдем по формуле полной вероятности:\n",
    "$$P(B)=P(B\\mid A_1)P(A_1) + P(B\\mid A_2)P(A_2) + P(B\\mid A_3)P(A_3) = 0.38 \\cdot 0.04 + 0.27 \\cdot 0.03 + 0.35 \\cdot 0.06 = 0.0443.$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формула Байеса\n",
    "На склад поступило 2 партии изделий: первая – 4000 штук, вторая – 6000 штук. Средний процент нестандартных изделий в первой партии составляет 20%, а во второй – 10%. Наудачу взятое со склада изделие оказалось стандартным. Найти вероятность того, что оно из первой партии.\n",
    "\n",
    "**Решение**\n",
    "\n",
    "Первая часть решения состоит в использовании формулы полной вероятности. \n",
    "- $A_1 =$ наудачу взятое изделие будет из 1-й партии;\n",
    "- $A_2 =$ наудачу взятое изделие будет из 2-й партии.\n",
    "\n",
    "Всего: $4000 + 6000 = 10000$ изделий на складе.\n",
    "$$P(A_1)=\\frac{4000}{10000}=0.4, P(A_1)=\\frac{6000}{10000}=0.6$$\n",
    "\n",
    "Рассмотрим зависимое событие: $B =$ наудачу взятое со склада изделие будет стандартным.\n",
    "\n",
    "В первой партии $100\\% – 20\\% = 80\\%$ стандартных изделий, поэтому: $P(B \\mid A_1) = \\frac{80}{100}=0.8$ – вероятность того, что наудачу взятое на складе изделие будет стандартным при условии, что оно принадлежит 1-й партии.\n",
    "\n",
    "Аналогично, во второй партии $100\\% – 10\\% = 90\\%$ стандартных изделий и $P(B \\mid A_2) = \\frac{90}{100}=0.9$ – вероятность того, что наудачу взятое на складе изделие будет стандартным при условии, что оно принадлежит 2-й партии.\n",
    "\n",
    "По формуле полной вероятности:\n",
    "$P(B)=P(B\\mid A_1)P(A_1) + P(B\\mid A_2)P(A_2) = 0.4 \\cdot 0.8 + 0.6 \\cdot 0.9 = 0.86$ – вероятность того, что наудачу взятое на складе изделие будет стандартным.\n",
    "\n",
    "Часть вторая. Пусть наудачу взятое со склада изделие оказалось стандартным. Эта фраза прямо прописана в условии, значит, событие $B$ произошло.\n",
    "\n",
    "По формулам Байеса:\n",
    "$$P(A_1 \\mid B) = \\frac{P(A_1) \\cdot P(B \\mid A_1)}{P(B)} = \\frac{0.4 \\cdot 0.8}{0.86} \\approx 0.37$$ – вероятность того, что выбранное стандартное изделие принадлежит 1-й партии.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парадокс Монти Холла\n",
    "Представьте, что вы стали участником игры, в которой вам нужно выбрать одну из трёх дверей. За одной из дверей находится автомобиль, за двумя другими дверями — козы. Вы выбираете одну из дверей, например, номер 1, после этого ведущий, который знает, где находится автомобиль, а где — козы, открывает одну из оставшихся дверей, например, номер 3, за которой находится коза. После этого он спрашивает вас — не желаете ли вы изменить свой выбор и выбрать дверь номер 2? Увеличатся ли ваши шансы выиграть автомобиль, если вы примете предложение ведущего и измените свой выбор?\n",
    "\n",
    "При этом:\n",
    "- автомобиль равновероятно размещён за любой из трёх дверей;\n",
    "- ведущий знает, где находится автомобиль;\n",
    "- ведущий в любом случае обязан открыть дверь с козой (но не ту, которую выбрал игрок) и предложить игроку изменить выбор;\n",
    "- если у ведущего есть выбор, какую из двух дверей открыть (то есть, игрок указал на верную дверь, и за обеими оставшимися дверями — козы), он выбирает любую из них с одинаковой вероятностью.\n",
    "\n",
    "\n",
    "| Дверь 1 | Дверь 2 | Дверь 3 | Результат, если менять выбор | Результат, если не менять выбор |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Авто | Коза | Коза | Коза | Авто |\n",
    "| Коза | Авто | Коза | Авто | Коза |\n",
    "| Коза | Коза | Авто | Авто | Коза |\n",
    "\n",
    "Для стратегии выигрыша важно следующее: если вы меняете выбор двери после действий ведущего, то вы выигрываете, если изначально выбрали проигрышную дверь. Это произойдёт с вероятностью $\\frac{2}{3}$, так как изначально выбрать проигрышную дверь можно 2 способами из 3.\n",
    "\n",
    "Но часто при решении этой задачи рассуждают примерно так: ведущий всегда в итоге убирает одну проигрышную дверь, и тогда вероятности появления автомобиля за двумя не открытыми становятся равны $\\frac{1}{2}$, вне зависимости от первоначального выбора. Но это неверно: хотя возможностей выбора действительно остаётся две, эти возможности (с учётом предыстории) не являются равновероятными. Это так, поскольку изначально все двери имели равные шансы быть выигрышными, но затем имели разные вероятности быть исключёнными.\n",
    "\n",
    "## Задачи\n",
    "1. Вероятность выпадения 6-х решек при 10 подбрасываниях монеты.\n",
    "2. Вероятность выпадения 6 очков 2 раза при 8 подбрасываниях кубика.\n",
    "3. Имеются три одинаковые урны. В первой урне находятся 5 белых и 3 черных шаров, во второй – только белые и в третьей – только черные шары. Наудачу выбирается одна урна и из неё наугад извлекается шар. Какова вероятность того, что этот шар чёрный?\n",
    "4. На склад поступило 2 партии изделий: первая – 8000 штук, вторая – 5000 штук. Средний процент нестандартных изделий в первой партии 15%, во второй – 23%. Наудачу взятое со склада изделие оказалось нестандартным. Найти вероятность того, что оно из второй партии.\n",
    "5. Электролампы изготавливаются на трех заводах. 1-й завод производит 30% общего количества ламп, 2-й – 55%, а 3-й – остальную часть. Продукция 1-го завода содержит 1% бракованных ламп, 2-го – 1,5%, 3-го – 2%. В магазин поступает продукция всех трех заводов. Купленная лампа оказалась с браком. Какова вероятность того, что она произведена 2-м заводом?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='chapter4'></a>\n",
    "# 3. Основы статистики \n",
    "Выборка — конечный набор прецедентов (объектов, случаев, событий, испытуемых, образцов, и т.п.), некоторым способом выбранных из множества всех возможных прецедентов, называемого генеральной совокупностью.\n",
    "\n",
    "## 3.1. Генеральная совокупность и выборка <a id='part4_1'></a>\n",
    "Случайная выборка\n",
    "Вероятностная модель порождения данных предполагает, что выборка из генеральной совокупности формируется случайным образом. Объём (длина) выборки $m$ считается произвольной, но фиксированной, неслучайной величиной.\n",
    "\n",
    "Формально это означает, что с генеральной совокупностью $X$ связывается вероятностное пространство $\\langle X^m,\\Sigma^m,{\\mathbb P}_m\\rangle$, где $X^m$ — множество всех выборок длины $m$, $\\Sigma^m$ — заданная на этом множестве сигма-алгебра событий, ${\\mathbb P}_m$ — вероятностная мера, как правило, неизвестная.\n",
    "\n",
    "Случайная выборка $x^m = (x_1,\\ldots,x_m)$ — это последовательность из m прецедентов, выбранная из множества $X^m$ согласно вероятностной мере $\\mathbb{P}_m$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1)<a id='part4_2'></a>\n",
    "## 3.2. Типы переменных.\n",
    "Признаковое пространство.\n",
    "\n",
    "Признаком называется отображение $f:\\; X\\to D_f$, где $D_f$ — множество допустимых значений признака. Если заданы признаки $f_1,\\dots,f_n$, то вектор ${\\mathbf x} = (f_1(x),\\dots,f_n(x))$ называется признаковым описанием объекта $x\\in X$. Признаковые описания допустимо отождествлять с самими объектами. При этом множество $X = D_{f_1}\\times\\dots\\times D_{f_n}$ называют признаковым пространством.\n",
    "\n",
    "В зависимости от множества $D_f$ признаки делятся на следующие типы:\n",
    "- бинарный признак: $D_f=\\{0,1\\}$\n",
    "    - наличие пакета у абонента \n",
    "    - наличие привязанной банковской карты у абонента\n",
    "- категориальный признак: $D_f$ — конечное неупорядоченное множество (т.е. элементы этого множества можно сравнивать только на равенство/совпадение; нельзя сравнивать на больше/меньше)\n",
    "    - регион абонента\n",
    "    - тарифная линейка абонента\n",
    "- порядковый признак: $D_f$ — конечное упорядоченное множество (т.е. можно сравнивать значения между собой на больше/меньше, но нельзя измерять расстояние между ними)\n",
    "    - тип населённого пункта (столица, город, деревня)\n",
    "    - модель телефона (от самой старой к самой новой)\n",
    "- вещественный признак: $D_f=\\mathbb{R}$ — множество вещественных чисел\n",
    "    - возраст абонента\n",
    "    - баланс абонента\n",
    "- множествозначный признак: $D_f$ − множество всех подмножеств (т.е. значения на объекте является подмножеством некоторого множества)\n",
    "    - все БМП абонента\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='part4_3'></a>\n",
    "## 3.3. Выборочные оценки характеристик распределений\n",
    "\n",
    "### Средние\n",
    "1. **Математическое ожидание**\n",
    "    - Для дискретной случайной величины – сумма всех значений, которые она принимает, взятая с весами, равными вероятностям этих значений:\n",
    "    $$\\mathbb{E}\\xi = \\sum_i a_i p_i$$\n",
    "    - Для непрерывной случайной величины – интеграл по всей области определения случайной величины от х, умноженного на f(x):\n",
    "    $$\\mathbb{E}\\xi=\\int_{-\\infty}^{+\\infty} xf(x)\\, dx$$\n",
    "2. **Медиана** – квантиль порядка 0.5, т.е. такое значение, что случайная величина $\\xi$ может попасть слева и справа от него с одинаковой вероятностью 0.5:\n",
    "$$\\text{Квартиль порядка } \\alpha \\in (0,1) - X_\\alpha :P(X \\leqslant X_\\alpha) \\geqslant \\alpha, P(X\\geqslant X_\\alpha)\\geqslant 1-\\alpha$$\n",
    "$$P(X\\leqslant med X)\\geqslant 0.5, P(X\\geqslant med X) \\geqslant 0.5$$\n",
    "3. **Мода**\n",
    "    - Для дискретной величины $\\xi$ – наиболее вероятное её значение (т.е. значение, у которого наибольшая вероятность):\n",
    "    - Для непрерывной случайной величины $\\xi$ – точка, в которой плотность распределения достигает максимума:\n",
    "\n",
    "### Оценка средних\n",
    "1. Оценка мат.ожидания равна выборочному среднему (т.е. просто среднеарифметическое по всем значениям выборки):\n",
    "    $$\\bar X = \\frac{1}{n}X_i$$\n",
    "2. Выборочная медиана. Выборка значений (из $N$ элементов) упорядочивается от меньшего к большему (вариационный ряд, состоящий из порядковых статистик). Далее берётся центральный элемент ряда (если $N$ нечётное) или среднее двух центральных элементов (если $N$ чётное).\n",
    "    $$m=\n",
    "    \\begin{equation}\n",
    "    \\begin{cases}\n",
    "      X_{(k)}, &n=2k+1\\\\\n",
    "      \\frac{X_{(k)} + X_{(k+1)}}{2}, &n=2k\n",
    "    \\end{cases}\\,.\n",
    "\\end{equation}$$\n",
    "3. Выборочная мода – максимум оценки плотности\n",
    "\n",
    "### Разброс\n",
    "1. Разброс значений случайной величины – то, насколько сильно они концентрируются вокруг мат.ожидания.\n",
    "2. **Дисперсия** – считается как математическое ожидание квадрата отклонения случайной величины от своего математического ожидания.\n",
    "    $$\\mathbb{D}X = \\mathbb{E}((\\xi - \\mathbb{E}\\xi)^2) = \\mathbb{E}\\xi^2 - (\\mathbb{E}\\xi)^2$$\n",
    "Корень из дисперсии - **среднеквадратическое отклонение** (стандартное отклонение)\n",
    "    $$\\sigma=\\sqrt{\\mathbb{D}}$$\n",
    "3. **Интерквантильный размах** – разность 75% и 25% квантилей (квартилей).\n",
    "    $$IQR = X_{0.75}-X_{0.25}$$\n",
    "    \n",
    "### Оценка разброса\n",
    "1. Выборочная дисперсия – среднее арифметическое квадратов отклонения от выборочного среднего:\n",
    "    $$S^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar X)^2$$\n",
    "2. Выборочный квантиль порядка $\\alpha$ – порядковая статистика, порядок которой равен целой части от $\\alpha*n: X_{([\\alpha n])}$.\n",
    "3. Выборочный интерквантильный размах – разность соответствующих порядковых статистик:\n",
    "    $$IQR = X_{([0.75 n])} - X_{([0.25 n])}$$\n",
    "    \n",
    "### Правило трёх сигм\n",
    "Правило трёх сигм — практически все значения нормально распределённой случайной величины лежат в интервале:\n",
    "$$(\\mu -3\\sigma ;\\mu +3\\sigma),$$\n",
    "где $\\mu =\\mathbb {E} \\xi$ — математическое ожидание и параметр нормальной случайной величины.\n",
    "\n",
    "Более точно — приблизительно с вероятностью $0.9973$ значение нормально распределённой случайной величины лежит в указанном интервале.\n",
    "\n",
    "<img src=\"img/Standard_deviation_diagram_(decimal_comma).svg\" width=\"600\">\n",
    "<center>График плотности вероятности нормального распределения и процент попадания случайной величины на отрезки,</center>\n",
    "<center>равные среднеквадратическому отклонению.</center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4_4'></a>[наверх](#chapter1)\n",
    "## 3.4. ЗБЧ, ЦПТ \n",
    "\n",
    "### Закон больших чисел\n",
    "Пусть задана бесконечная последовательность независимых одинаково распределённых случайных величин $\\xi_1, \\xi_2, \\dots, \\xi_n, \\dots,$ для которых существует математическое ожидание $\\mathbb{E} \\xi_i = \\mu$ и дисперсия $\\mathbb{D}\\xi_i = \\sigma ^2$. Тогда для любого $\\varepsilon>0$ верно\n",
    "$$\\lim_{n \\rightarrow \\infty} P \\left(\\left|\\frac{1}{n} \\sum_{i=1}^n \\xi_i - \\mu \\right| \\geqslant \\varepsilon \\right)$$\n",
    "\n",
    "Смысл закона больших чисел: при возрастании числа слагаемых независимых одинаково распределённых случайных величин среднее арифметическое этих слагаемых мало отличается от математического ожидания $\\mu$. \n",
    "\n",
    "### Центральная предельная теорема\n",
    "Пусть $\\xi_1, \\xi_2, \\dots, \\xi_n, \\dots$ - независимые одинаково распределённые случайные величины с $\\mathbb{E} \\xi_i = \\mu$ и $\\mathbb{D}\\xi_i = \\sigma ^2$. Тогда для любого числа x верно:\n",
    "$$\\lim_{n \\rightarrow \\infty} P \\left(\\frac{\\xi_1 + \\xi_2 + \\dots, \\xi_n - n\\mu}{\\sigma \\sqrt(n)} <x \\right) = \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int _{-\\infty}^x e^{-\\frac{y^2}{2}} dy$$\n",
    "\n",
    "Смысл центральной предельной теоремы: сумма $n$ случайных величин при линейном преобразовании и стремлении $n$ к бесконечности ведёт себя почти как случайная величина со стандартным нормальным распределением $N(0,1)$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1)<a id='part4_5'></a>\n",
    "## 3.5. Доверительные интервалы \n",
    "Доверительный интерал — термин, используемый в математической статистике при интервальной оценке статистических параметров, более предпочтительной при небольшом объёме выборки, чем точечная. Доверительным называют интервал, который покрывает неизвестный параметр с заданной надёжностью.\n",
    "\n",
    "Доверительным интервалом параметра $\\theta$  распределения случайной величины $X$ с уровнем доверия $p$, порождённым выборкой $(x_{1},\\ldots ,x_{n})$, называется интервал с границами $l(x_{1},\\ldots ,x_{n})$ и $u(x_{1},\\ldots ,x_{n})$, которые являются реализациями случайных величин $L(X_{1},\\ldots ,X_{n})$ и $U(X_{1},\\ldots ,X_{n})$, таких, что\n",
    "\n",
    "$$P (L\\leqslant \\theta \\leqslant U)=p.$$\n",
    "\n",
    "### Примеры\n",
    "Доверительный интервал для математического ожидания нормальной выборки\n",
    "#### Случай известной дисперсии\n",
    "Пусть $X_{1},\\ldots ,X_{n}\\sim \\mathrm {N} (\\mu ,\\sigma ^{2})$ — независимая выборка из нормального распределения, где $\\sigma ^{2}$ — известная дисперсия. Определим произвольное $\\alpha \\in [0,1]$ и построим доверительный интервал для неизвестного среднего $\\mu$.\n",
    "$$\\mathbb {P} \\left({\\bar {X}}-z_{1-{\\frac {\\alpha }{2}}}{\\frac {\\sigma }{\\sqrt {n}}}\\leq \\mu \\leq {\\bar {X}}+z_{1-{\\frac {\\alpha }{2}}}{\\frac {\\sigma }{\\sqrt {n}}}\\right)=1-\\alpha,$$\n",
    "где $z_{\\alpha }$ — $\\alpha$-квантиль стандартного нормального распределения.\n",
    "\n",
    "#### Случай неизвестной дисперсии\n",
    "Пусть $X_{1},\\ldots ,X_{n}\\sim \\mathrm {N} (\\mu ,\\sigma ^{2})$ — независимая выборка из нормального распределения, где $\\mu ,\\sigma ^{2}$ — неизвестные константы. Построим доверительный интервал для неизвестного среднего $\\mu$.\n",
    "$$\\mathbb {P} \\left({\\bar {X}}-t_{1-{\\frac {\\alpha }{2}},n-1}{\\frac {S}{\\sqrt {n}}}\\leq \\mu \\leq {\\bar {X}}+t_{1-{\\frac {\\alpha }{2}},n-1}{\\frac {S}{\\sqrt {n}}}\\right)=1-\\alpha ,$$\n",
    "где $S$ — несмещённое выборочное стандартное отклонение, $t_{\\alpha ,n-1}$ — $\\alpha$-квантили распределения Стьюдента. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='part4_6'></a>\n",
    "## 3.6. Идея статистического вывода, p-уровень значимости\n",
    "Статистический вывод — обобщение информации из выборки для получения представления о свойствах генеральной совокупности.\n",
    "\n",
    "В статистическом выводе на основе случайной выборки делаются предположения относительно генеральной совокупности, используя данные о ней. В более общем смысле, данные о некотором случайном процессе, полученные из его наблюдения в течение конечного промежутка времени. Результатом статистического вывода является статистическое суждение, например: точечная оценка, доверительный интервал, отвержение гипотезы.\n",
    "\n",
    "Проверка статистических гипотез является содержанием одного из обширных классов задач математической статистики. Статистическая гипотеза — предположение о виде распределения и свойствах случайной величины, которое можно подтвердить или опровергнуть применением статистических методов к данным выборки.\n",
    "\n",
    "Эксперимент – записываются предсказания, генерируются события, проверяется правильность предсказаний.\n",
    "\n",
    "### Проверка гипотез\n",
    "Имеется некоторая выборка размера $n$ из случайной величины $Х$, которая имеет неизвестное распределение $P$:\n",
    "$$X^n=(X_1, \\dots, X_n), X \\sim P$$\n",
    "Имеются следующие гипотезы об этом распределении $P$:\n",
    "- Нулевая гипотеза $H_0$ – например, $P$ лежит в некотором семействе $\\omega$:\n",
    "    $$H_0: P \\in \\omega$$\n",
    "- Альтернативная гипотеза $H_1$ – $P$ не лежит в некотором семействе $\\omega$:\n",
    "    $$H_1: P \\notin \\omega$$\n",
    "    \n",
    "Глядя на собранные данные, нужно проверить, какая из гипотез наиболее вероятна. Для этого используется некоторая статистика $Т$, которая обладает свойством – если справедлива $H_0$, то мы точно знаем, какое у этой статистики распределение:\n",
    "    $$T(X^n) \\sim F(x) \\text{, при } H_0$$\n",
    "    $$T(X^n) \\not\\sim F(x) \\text{, при } H_1$$\n",
    "Если верна $H_1$, то статистика $Т$ имеет распределение не $F(x)$, а какое-то другое.\n",
    "- $F(x)$ – нулевое распределение статистики.\n",
    "- Пара $T$ и $F(x)$ образует статистический критерий для проверки нулевой гипотезы против альтернативной.\n",
    "- $t$ – значение статистики на полученных данных.\n",
    "\n",
    "**$p$-value** - достигаемый уровень значимости (вероятность при справедливости $H_0$ можно получить значения статистики $T$ равное $t$ или больше)\n",
    "    $$p=P(T\\geqslant t | H_0).$$\n",
    "Зная нулевое распределение статистики и значение статистики, которое реализовалось в эксперименте, можно посчитать $p$-value.\n",
    "Если критическими (т.е. соответствующими альтернативе) являются большие значения статистики, $p$-value равен интегралу от плотности нулевого распределения по «правому хвосту» на диапазоне $[t, +\\infty]$. Если полученное $p$-value мало – значит, данные свидетельствуют против $H_0$ в пользу $H_1$.\n",
    "$p$-value сравнивается с порогом $\\alpha$ (**уровень значимости**): $H_0$ отвергается в пользу $H_1$ при $p \\leqslant \\alpha$. Чаще всего, $\\alpha=0.05$ (т.е. $95\\%$ доверительный интервал).\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[наверх](#chapter1) <a id='part4_7'></a>\n",
    "## 3.7. A/B тесты \n",
    "\n",
    "### Что такое A/B тестирование, зачем и для кого это нужно?\n",
    "\n",
    "A/B-тестирование — это неотъемлемая часть процесса работы над продуктом. Это эксперимент, который позволяет сравнить две версии чего-либо, чтобы проверить гипотезы и определить, какая версия лучше. Какая цена лучше, какой дизайн более привлекательный, какая навигация лучше, какой офер более интересен абоненту? Но это не всегда история про успешные тестирования и постоянный рост. В первую очередь надо думать, как не сделать проект хуже. \n",
    "\n",
    "Кому нужно A/B-тестирование? - всем, кто хочет улучшать свой продукт, делать его удобнее и приятнее для целевой аудитории. Например,\n",
    "- Продакт-менеджеры могут тестировать изменения ценовых моделей, направленные на повышение доходов, или оптимизацию части воронки продаж для увеличения конверсии.\n",
    "- Маркетологи могут тестировать изображения, призывы к действию (call-to-action) или практически любые другие элементы маркетинговой кампании или рекламы с точки зрения улучшения метрик.\n",
    "- Продуктовые дизайнеры могут тестировать дизайнерские решения или использовать результаты тестирования для того, чтобы перед внедрением определить, будет ли удобно пользоваться новой функцией.\n",
    "\n",
    "### Основные шаги A/B тестирования:\n",
    "1. определить цель\n",
    "2. выбрать метрику: \n",
    "    - конверсия,\n",
    "    - экономические метрики,\n",
    "    - поведенческие факторы,\n",
    "    - ...\n",
    "3. выдвинуть гипотезу\n",
    "4. настроить эксперимент\n",
    "5. запустить эксперимент\n",
    "6. проанализировать результаты\n",
    "\n",
    "### Как проводить A/B тесты?\n",
    "\n",
    "Нужно определить две гипотезы, которые помогут понять, является ли наблюдаемая разница между версией A (изначальной) и версией B (новой, которую вы хотите проверить) случайностью или результатом изменений, которые вы произвели.\n",
    "\n",
    "Нулевая гипотеза предполагает, что результаты, А и В на самом деле не отличаются и что наблюдаемые различия случайны. Мы надеемся опровергнуть эту гипотезу.\n",
    "Альтернативная гипотеза — это гипотеза о том, что B отличается от A, и вы хотите сделать вывод об её истинности.\n",
    "Можно проводить односторонний или двусторонний тест. Односторонний тест позволяет обнаружить изменение в одном направлении, в то время как двусторонний тест позволяет обнаружить изменение по двум направлениям (как положительное, так и отрицательное).  \n",
    "\n",
    "Для того, чтобы тест выдавал корректные результаты: \n",
    "- Создайте новую версию (B), отражающую изменения, которые вы хотите протестировать.\n",
    "- Определите контрольную и экспериментальную группы. Каких пользователей вы хотите протестировать: всех пользователей на всех платформах или только пользователей из одной страны? Определите группу испытуемых, отобрав их по типам пользователей, платформе, географическим показателям и т. п. Затем определите, какой процент исследуемой группы составляет контрольная группа (группа, видящая версию A), а какой процент — экспериментальная группа (группа, видящая версию B).\n",
    "- Убедитесь, что пользователи будут видеть версии A и B в случайном порядке. Это значит, у каждого пользователя будет равный шанс получить ту или иную версию.\n",
    "- Определите уровень статистической значимости ($\\alpha$). Это уровень риска, который вы принимаете при ошибках первого рода (отклонение нулевой гипотезы, если она верна), обычно $\\alpha = 0.05$. Это означает, что в $5\\%$ случаев вы будете обнаруживать разницу между A и B, которая на самом деле обусловлена случайностью. Чем ниже выбранный вами уровень значимости, тем ниже риск того, что вы обнаружите разницу, вызванную случайностью.\n",
    "- Определите минимальный размер выборки. Наличие достаточно большого размера выборки важно для обеспечения статистически значимых результатов.\n",
    "- Определите временные рамки. Возьмите общий размер выборки, необходимый вам для тестирования каждой версии, и разделите его на ваш ежедневный трафик, так вы получите количество дней, необходимое для проведения теста.\n",
    "\n",
    "Сколько должен длиться эксперимент? Сколько пользователей должно быть в тестовой выборке?\n",
    "\n",
    "Для начала нужно определить следующее:\n",
    "- минимальный размер эффекта, который мы хотим померить (насколько большие отклонения от метрики, которая, мы предполагаем, сохраниться по умолчанию, мы хотим \"замечать\")\n",
    "- допустимые вероятности ошибок первого и второго рода (ошибка первого рода — ситуация, когда отвергнута верная нулевая гипотеза и ошибка второго рода — ситуация, когда принята неверная нулевая гипотеза); стоимости этих ошибок могут существенно отличаться, и в зависимости от этого нужно выбирать пороги.\n",
    "    \n",
    "С помощью калькулятора мощности выбранного статистического критерия можно будет расчитать размеры выборки (калькуляторы [здесь](https://www.optimizely.com/sample-size-calculator/?conversion=3&effect=20&significance=95) и [здесь](https://www.abtasty.com/sample-size-calculator/)).\n",
    "\n",
    "[Калькулятор достоверности A/B тестирования](https://yandex.ru/adv/statvalue).\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
